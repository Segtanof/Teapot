{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customs Brokers']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the folder name with current date and time\n",
    "folder_name = 'results/task_match_'+datetime.now().strftime(\"%d%m_%H%M\")+\"/\"\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\",\n",
    "                    handlers=[logging.FileHandler(\"execution_log.log\"), logging.StreamHandler()])\n",
    "\n",
    "# read dataset and drop columns\n",
    "job_statements = pd.read_excel(\"datasets/task_statements.xlsx\")\n",
    "job_statements.columns = job_statements.columns.str.lower()\n",
    "job_statements = job_statements.drop(labels=[\"incumbents responding\",\"date\",\"domain source\"], axis=1).rename(columns={\"o*net-soc code\":\"code\", \"task type\":\"type\", \"task id\": \"id\", \"task\":\"ref_task\"})\n",
    "job_statements = job_statements[job_statements[\"type\"].notna()]\n",
    "job_statements[\"ind\"] = job_statements[\"code\"].str[:2]\n",
    "job_statements = job_statements.groupby(\"title\").agg({\"ref_task\":list, \"ind\": \"first\"}).reset_index().sort_values(\"ind\")\n",
    "sampled_occupation = job_statements.groupby('ind', group_keys=False).sample(frac=0.05, random_state=1) #43 samples\n",
    "\n",
    "#for trial\n",
    "trial_df = sampled_occupation.sample(1, random_state= 1)\n",
    "test_sample_list =[trial_df.iloc[x][\"title\"] for x in range(len(trial_df))]\n",
    "test_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_des (title):\n",
    "    task_list = sampled_occupation.query(\"title == @title\")[\"ref_task\"].iloc[0]\n",
    "    return task_list\n",
    "\n",
    "def task_gen(title, model, system=None):  # [unchanged]\n",
    "    ref_task_count = len(get_des(title))\n",
    "    json_schema = {\"type\": \"object\", \"properties\": {\"occupation\": {\"type\": \"string\"}, \"tasks\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"minItems\": ref_task_count, \"maxItems\": ref_task_count}}, \"required\": [\"occupation\", \"tasks\"]}\n",
    "    query = f\"List exactly {ref_task_count} unique task statements that the occupation '{title}' would perform at work.\"\n",
    "    prompt_template = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")] if system else [(\"human\", \"{input}\")])\n",
    "    llm = model.with_structured_output(schema=json_schema, method=\"json_schema\")\n",
    "    prompt = prompt_template.invoke({\"input\": query})\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        tasks = response[\"tasks\"]\n",
    "        if len(tasks) != ref_task_count or len(set(tasks)) < len(tasks):\n",
    "            logging.warning(f\"Task issues for {title}: count {len(tasks)}/{ref_task_count}, uniques {len(set(tasks))}\")\n",
    "        return tasks\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed for {title}: {e}\")\n",
    "        return [f\"Error: Task {i+1} for {title}\" for i in range(ref_task_count)]\n",
    "\n",
    "def process_title(args):\n",
    "    title, model_config, prompt = args\n",
    "    model = ChatOllama(**model_config)\n",
    "    start_time = datetime.now()\n",
    "    tasks = task_gen(title, model, system=prompt)\n",
    "    logging.info(f\"Single inference for {title}, duration: {datetime.now() - start_time}\")\n",
    "    return title, tasks\n",
    "\n",
    "def preProcessText(text):  # [unchanged]\n",
    "    processed = []\n",
    "    for doc in text:\n",
    "        if not isinstance(doc, str): doc = str(doc)\n",
    "        doc = re.sub(r\"\\\\n|\\W|\\d\", \" \", doc)\n",
    "        doc = re.sub(r'\\s+[a-z]\\s+|^[a-z]\\s+|\\s+', \" \", doc)\n",
    "        doc = re.sub(r'^\\s|\\s$', \"\", doc)\n",
    "        processed.append(doc.lower())\n",
    "    return processed\n",
    "\n",
    "def sbert_batch(ref_list, gen_list):\n",
    "    sim_model = SentenceTransformer(\"all-mpnet-base-v2\", similarity_fn_name=\"cosine\", device=\"cuda\")\n",
    "    embeddings_ref = sim_model.encode(ref_list, batch_size=32, convert_to_tensor=True)\n",
    "    embeddings_gen = sim_model.encode(gen_list, batch_size=32, convert_to_tensor=True)\n",
    "    return sim_model.similarity(embeddings_ref, embeddings_gen).cpu().numpy()\n",
    "\n",
    "def match_batch(ref_lists, gen_lists):\n",
    "    results = []\n",
    "    for ref_tasks, gen_tasks in zip(ref_lists, gen_lists):\n",
    "        ref_clean = preProcessText(ref_tasks)\n",
    "        gen_clean = preProcessText(gen_tasks)\n",
    "        matrix = sbert_batch(ref_clean, gen_clean)\n",
    "        row_ind, col_ind = linear_sum_assignment(1 - matrix)\n",
    "        avg_score = np.mean(matrix[row_ind, col_ind])\n",
    "        results.append((avg_score, matrix.tolist(), row_ind.tolist(), col_ind.tolist()))\n",
    "    return results\n",
    "\n",
    "def match_batch_parallel(ref_lists, gen_lists, num_processes=8):\n",
    "    chunk_size = max(1, len(ref_lists) // num_processes)\n",
    "    chunks = [(ref_lists[i:i + chunk_size], gen_lists[i:i + chunk_size]) for i in range(0, len(ref_lists), chunk_size)]\n",
    "    \n",
    "    def process_chunk(chunk):\n",
    "        refs, gens = chunk\n",
    "        return match_batch(refs, gens)\n",
    "    \n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        chunk_results = pool.map(process_chunk, chunks)\n",
    "    \n",
    "    # Flatten results\n",
    "    results = []\n",
    "    for chunk in chunk_results:\n",
    "        results.extend(chunk)\n",
    "    return results\n",
    "\n",
    "# Replace in main script:\n",
    "def apply_match_batch(df):\n",
    "    ref_lists = df[\"ref_task\"].tolist()\n",
    "    gen_lists = df[\"gen_task\"].tolist()\n",
    "    results = match_batch_parallel(ref_lists, gen_lists, num_processes=8)\n",
    "    scores, matrices, ref_orders, gen_orders = zip(*results)\n",
    "    df[\"score\"] = scores\n",
    "    df[\"matrix\"] = matrices\n",
    "    df[\"ref_order\"] = ref_orders\n",
    "    df[\"gen_order\"] = gen_orders\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    {\"model\": \"llama3.2\", \"temperature\": 1, \"base_url\": \"http://127.0.0.1:11434\"},\n",
    "    {\"model\": \"llama3.1\", \"temperature\": 1, \"base_url\": \"http://127.0.0.1:11434\"}\n",
    "]\n",
    "prompts = {\n",
    "    \"no_prompt\": None,\n",
    "    \"prompt1\": \"You are an expert of this occupation: \\\"{title}\\\". Your task is to generate clear and concise task descriptions...\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 18:41:13,681 - Script started\n",
      "2025-03-19 18:41:13,683 - Processing model: llama3.2\n",
      "2025-03-19 18:41:14,511 - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "llama3.2-no_prompt-0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "logging.info(\"Script started\")\n",
    "for model_config in model_configs:\n",
    "    model_name = model_config[\"model\"]\n",
    "    logging.info(f\"Processing model: {model_name}\")\n",
    "    model = ChatOllama(**model_config)\n",
    "    model.invoke(\"Warm-up prompt\")\n",
    "\n",
    "    for name, prompt in prompts.items():\n",
    "        if prompt:\n",
    "            start_time = datetime.now()\n",
    "            with open(f\"{folder_name}/sys_prompt.txt\", \"a\") as f:\n",
    "                f.write(prompt + \"\\n\")\n",
    "            logging.info(f\"Wrote prompt {name}, duration: {datetime.now() - start_time}\")\n",
    "\n",
    "        all_results_df = trial_df.copy()\n",
    "        all_results_df[\"gen_task\"] = [None] * len(all_results_df)\n",
    "        all_results_df[\"iteration\"] = None\n",
    "\n",
    "        for i in range(2):\n",
    "            start_time = datetime.now()\n",
    "            with Pool(processes=1) as pool:\n",
    "                results = list(tqdm(\n",
    "                    pool.imap_unordered(process_title, [(title, model_config, prompt) for title in test_sample_list]),\n",
    "                    total=len(test_sample_list), desc=f\"{model_name}-{name}-{i}\"\n",
    "                ))\n",
    "            logging.info(f\"Multiprocessing for {model_name}-{name}-{i}, duration: {datetime.now() - start_time}\")\n",
    "\n",
    "            temp_df = trial_df.copy()\n",
    "            for title, tasks in results:\n",
    "                temp_df.loc[temp_df[\"title\"] == title, \"gen_task\"] = pd.Series([tasks]).values\n",
    "            temp_df[\"iteration\"] = i\n",
    "            all_results_df = pd.concat([all_results_df, temp_df], ignore_index=True)\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        all_results_df = apply_match_batch(all_results_df)\n",
    "        logging.info(f\"Batch matching for {model_name}-{name}, duration: {datetime.now() - start_time}\")\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        all_results_df = all_results_df.reset_index(drop=True)\n",
    "        with open(f\"{folder_name}/{model_name}_{name}_results.json\", \"w\") as f:\n",
    "            f.write(all_results_df.to_json(index=True))\n",
    "        logging.info(f\"Wrote results JSON for {model_name}-{name}, duration: {datetime.now() - start_time}\")\n",
    "\n",
    "logging.info(\"Script completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 22:49:06,179 - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-03-19 22:49:12,807 - Single inference for Customs Brokers, duration: 0:00:06.993114\n"
     ]
    }
   ],
   "source": [
    "x = process_title((\"Customs Brokers\", {\"model\": \"llama3.2\"}, \"You are an expert of this occupation. Your task is to generate clear and concise task descriptions.Start with a verb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_gen(title, model, system=None):  # [unchanged]\n",
    "    ref_task_count = len(get_des(title))\n",
    "    json_schema = {\"type\": \"object\", \"properties\": {\"occupation\": {\"type\": \"string\"}, \"tasks\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"minItems\": ref_task_count, \"maxItems\": ref_task_count}}, \"required\": [\"occupation\", \"tasks\"]}\n",
    "    query = \"List exactly \"+ str(ref_task_count) +\" unique task statements that the occupation \" + title + \"would perform at work.\"\n",
    "    prompt_template = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")] if system else [(\"human\", \"{input}\")])\n",
    "    llm = model.with_structured_output(schema=json_schema, method=\"json_schema\")\n",
    "    prompt = prompt_template.invoke({\"input\": query, \"title\": title})\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        tasks = response[\"tasks\"]\n",
    "        if len(tasks) != ref_task_count or len(set(tasks)) < len(tasks):\n",
    "            logging.warning(f\"Task issues for {title}: count {len(tasks)}/{ref_task_count}, uniques {len(set(tasks))}\")\n",
    "        return tasks\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed for {title}: {e}\")\n",
    "        return [f\"Error: Task {i+1} for {title}\" for i in range(ref_task_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are an expert of this occupation: \\\"{title}\\\". Your task is to generate clear and concise task descriptions...\"\n",
    "# Invoke with both required variables\n",
    "# prompt = prompt_template.invoke({\"input\": query, \"title\": title})\n",
    "# query = \"List exactly 10 unique task statements that the occupation '{title}' would perform at work.\"\n",
    "\n",
    "# # Ensure the system message is included correctly\n",
    "# prompt_template = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", system),\n",
    "#     (\"human\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "# # Invoke with both required variables\n",
    "# prompt = prompt_template.invoke({\"input\": query, \"title\": title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 23:36:37,284 - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "x = task_gen(title, ChatOllama(model=\"llama3.2\"), system=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessText(text):  # [unchanged]\n",
    "    processed = []\n",
    "    for doc in text:\n",
    "        if not isinstance(doc, str): doc = str(doc)\n",
    "        doc = re.sub(r\"\\\\n|\\W|\\d\", \" \", doc)\n",
    "        doc = re.sub(r'\\s+[a-z]\\s+|^[a-z]\\s+|\\s+', \" \", doc)\n",
    "        doc = re.sub(r'^\\s|\\s$', \"\", doc)\n",
    "        processed.append(doc.lower())\n",
    "    return processed\n",
    "\n",
    "def sbert_batch(ref_list, gen_list):\n",
    "    sim_model = SentenceTransformer(\"all-mpnet-base-v2\", similarity_fn_name=\"cosine\", device=\"cuda\")\n",
    "    embeddings_ref = sim_model.encode(ref_list, batch_size=32, convert_to_tensor=True)\n",
    "    embeddings_gen = sim_model.encode(gen_list, batch_size=32, convert_to_tensor=True)\n",
    "    return sim_model.similarity(embeddings_ref, embeddings_gen).cpu().numpy()\n",
    "\n",
    "def match_batch(ref_lists, gen_lists):\n",
    "    results = []\n",
    "    for ref_tasks, gen_tasks in zip(ref_lists, gen_lists):\n",
    "        ref_clean = preProcessText(ref_tasks)\n",
    "        gen_clean = preProcessText(gen_tasks)\n",
    "        matrix = sbert_batch(ref_clean, gen_clean)\n",
    "        row_ind, col_ind = linear_sum_assignment(1 - matrix)\n",
    "        avg_score = np.mean(matrix[row_ind, col_ind])\n",
    "        results.append((avg_score, matrix.tolist(), row_ind.tolist(), col_ind.tolist()))\n",
    "    return results\n",
    "\n",
    "def process_chunk(chunk):\n",
    "        refs, gens = chunk\n",
    "        return match_batch(refs, gens)\n",
    "\n",
    "def match_batch_parallel(ref_lists, gen_lists, num_processes=2):\n",
    "    chunk_size = max(1, len(ref_lists) // num_processes)\n",
    "    chunks = [(ref_lists[i:i + chunk_size], gen_lists[i:i + chunk_size]) for i in range(0, len(ref_lists), chunk_size)]\n",
    "    \n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        chunk_results = pool.map(process_chunk, chunks)\n",
    "    \n",
    "    # Flatten results\n",
    "    results = []\n",
    "    for chunk in chunk_results:\n",
    "        results.extend(chunk)\n",
    "    return results\n",
    "\n",
    "# Replace in main script:\n",
    "def apply_match_batch(df):\n",
    "    ref_lists = df[\"ref_task\"].tolist()\n",
    "    gen_lists = df[\"gen_task\"].tolist()\n",
    "    results = match_batch_parallel(ref_lists, gen_lists, num_processes=8)\n",
    "    # results = match_batch(ref_lists, gen_lists)\n",
    "    scores, matrices, ref_orders, gen_orders = zip(*results)\n",
    "    df[\"score\"] = scores\n",
    "    df[\"matrix\"] = matrices\n",
    "    df[\"ref_order\"] = ref_orders\n",
    "    df[\"gen_order\"] = gen_orders\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'results/task_match_2003_0105'\n",
    "all_results_df = pd.read_json(folder_name + \"/mistral_no_prompt_results.json\").dropna()\n",
    "all_results_df = apply_match_batch(all_results_df)\n",
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.92065364,\n",
       "  [[1.0000001192092896,\n",
       "    0.33057382702827454,\n",
       "    0.37065088748931885,\n",
       "    0.37419450283050537,\n",
       "    0.33057382702827454,\n",
       "    0.37065088748931885,\n",
       "    0.37762686610221863,\n",
       "    0.33057382702827454,\n",
       "    0.24337777495384216,\n",
       "    0.311149001121521],\n",
       "   [0.33057382702827454,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.41770994663238525,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.6471920609474182,\n",
       "    1.0,\n",
       "    0.23772287368774414,\n",
       "    0.5197737216949463],\n",
       "   [0.37065088748931885,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.3942541778087616,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.4575287401676178,\n",
       "    0.4602755904197693,\n",
       "    0.1967637538909912,\n",
       "    0.4293457269668579],\n",
       "   [0.37419450283050537,\n",
       "    0.41770994663238525,\n",
       "    0.3942541778087616,\n",
       "    1.0000001192092896,\n",
       "    0.41770994663238525,\n",
       "    0.3942541778087616,\n",
       "    0.45839443802833557,\n",
       "    0.41770994663238525,\n",
       "    0.20226545631885529,\n",
       "    0.3711409568786621],\n",
       "   [0.33057382702827454,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.41770994663238525,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.6471920609474182,\n",
       "    1.0,\n",
       "    0.23772287368774414,\n",
       "    0.5197737216949463],\n",
       "   [0.37065088748931885,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.3942541778087616,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.4575287401676178,\n",
       "    0.4602755904197693,\n",
       "    0.1967637538909912,\n",
       "    0.4293457269668579],\n",
       "   [0.37762686610221863,\n",
       "    0.6471920609474182,\n",
       "    0.4575287401676178,\n",
       "    0.45839443802833557,\n",
       "    0.6471920609474182,\n",
       "    0.4575287401676178,\n",
       "    1.0,\n",
       "    0.6471920609474182,\n",
       "    0.17308847606182098,\n",
       "    0.6375619769096375],\n",
       "   [0.33057382702827454,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.41770994663238525,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.6471920609474182,\n",
       "    1.0,\n",
       "    0.23772287368774414,\n",
       "    0.5197737216949463],\n",
       "   [0.24337777495384216,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.20226545631885529,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.17308847606182098,\n",
       "    0.23772287368774414,\n",
       "    1.0,\n",
       "    0.2065361589193344],\n",
       "   [0.24337777495384216,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.20226545631885529,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.17308847606182098,\n",
       "    0.23772287368774414,\n",
       "    1.0,\n",
       "    0.2065361589193344]],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " (0.917964,\n",
       "  [[1.0000001192092896,\n",
       "    0.33057382702827454,\n",
       "    0.37065088748931885,\n",
       "    0.37419450283050537,\n",
       "    0.33057382702827454,\n",
       "    0.37065088748931885,\n",
       "    0.37762686610221863,\n",
       "    0.33057382702827454,\n",
       "    0.24337777495384216,\n",
       "    0.3363737463951111],\n",
       "   [0.33057382702827454,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.41770994663238525,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.6471920609474182,\n",
       "    1.0,\n",
       "    0.23772287368774414,\n",
       "    0.6397554874420166],\n",
       "   [0.37065088748931885,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.3942541778087616,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.4575287401676178,\n",
       "    0.4602755904197693,\n",
       "    0.1967637538909912,\n",
       "    0.43051937222480774],\n",
       "   [0.37419450283050537,\n",
       "    0.41770994663238525,\n",
       "    0.3942541778087616,\n",
       "    1.0000001192092896,\n",
       "    0.41770994663238525,\n",
       "    0.3942541778087616,\n",
       "    0.45839443802833557,\n",
       "    0.41770994663238525,\n",
       "    0.20226545631885529,\n",
       "    0.41094136238098145],\n",
       "   [0.33057382702827454,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.41770994663238525,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.6471920609474182,\n",
       "    1.0,\n",
       "    0.23772287368774414,\n",
       "    0.6397554874420166],\n",
       "   [0.37065088748931885,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.3942541778087616,\n",
       "    0.4602755904197693,\n",
       "    1.0000001192092896,\n",
       "    0.4575287401676178,\n",
       "    0.4602755904197693,\n",
       "    0.1967637538909912,\n",
       "    0.43051937222480774],\n",
       "   [0.37762686610221863,\n",
       "    0.6471920609474182,\n",
       "    0.4575287401676178,\n",
       "    0.45839443802833557,\n",
       "    0.6471920609474182,\n",
       "    0.4575287401676178,\n",
       "    1.0,\n",
       "    0.6471920609474182,\n",
       "    0.17308847606182098,\n",
       "    0.8036168813705444],\n",
       "   [0.33057382702827454,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.41770994663238525,\n",
       "    1.0,\n",
       "    0.4602755904197693,\n",
       "    0.6471920609474182,\n",
       "    1.0,\n",
       "    0.23772287368774414,\n",
       "    0.6397554874420166],\n",
       "   [0.24337777495384216,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.20226545631885529,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.17308847606182098,\n",
       "    0.23772287368774414,\n",
       "    1.0,\n",
       "    0.17963983118534088],\n",
       "   [0.24337777495384216,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.20226545631885529,\n",
       "    0.23772287368774414,\n",
       "    0.1967637538909912,\n",
       "    0.17308847606182098,\n",
       "    0.23772287368774414,\n",
       "    1.0,\n",
       "    0.17963983118534088]],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " (0.6146653,\n",
       "  [[0.3496246039867401,\n",
       "    0.40948644280433655,\n",
       "    0.40948644280433655,\n",
       "    0.24337777495384216,\n",
       "    0.3363737463951111,\n",
       "    0.311149001121521,\n",
       "    0.3340495824813843],\n",
       "   [0.5790420770645142,\n",
       "    0.372037798166275,\n",
       "    0.372037798166275,\n",
       "    0.23772288858890533,\n",
       "    0.6397554874420166,\n",
       "    0.5197737216949463,\n",
       "    0.43755578994750977],\n",
       "   [0.4012153744697571,\n",
       "    0.481334388256073,\n",
       "    0.481334388256073,\n",
       "    0.1967637687921524,\n",
       "    0.43051937222480774,\n",
       "    0.4293457269668579,\n",
       "    0.437362939119339],\n",
       "   [0.4214903712272644,\n",
       "    0.37785646319389343,\n",
       "    0.37785646319389343,\n",
       "    0.20226545631885529,\n",
       "    0.41094136238098145,\n",
       "    0.3711409568786621,\n",
       "    0.3570364713668823],\n",
       "   [0.5790420770645142,\n",
       "    0.372037798166275,\n",
       "    0.372037798166275,\n",
       "    0.23772288858890533,\n",
       "    0.6397554874420166,\n",
       "    0.5197737216949463,\n",
       "    0.43755578994750977],\n",
       "   [0.4012153744697571,\n",
       "    0.481334388256073,\n",
       "    0.481334388256073,\n",
       "    0.1967637687921524,\n",
       "    0.43051937222480774,\n",
       "    0.4293457269668579,\n",
       "    0.437362939119339],\n",
       "   [0.6061398386955261,\n",
       "    0.4177130460739136,\n",
       "    0.4177130460739136,\n",
       "    0.17308847606182098,\n",
       "    0.8036168813705444,\n",
       "    0.6375619769096375,\n",
       "    0.41678208112716675],\n",
       "   [0.5790420770645142,\n",
       "    0.372037798166275,\n",
       "    0.372037798166275,\n",
       "    0.23772288858890533,\n",
       "    0.6397554874420166,\n",
       "    0.5197737216949463,\n",
       "    0.43755578994750977],\n",
       "   [0.23027460277080536,\n",
       "    0.2560599744319916,\n",
       "    0.2560599744319916,\n",
       "    1.0,\n",
       "    0.17963983118534088,\n",
       "    0.2065361589193344,\n",
       "    0.2152537703514099],\n",
       "   [0.23027460277080536,\n",
       "    0.2560599744319916,\n",
       "    0.2560599744319916,\n",
       "    1.0,\n",
       "    0.17963983118534088,\n",
       "    0.2065361589193344,\n",
       "    0.2152537703514099]],\n",
       "  [1, 2, 4, 5, 6, 7, 8],\n",
       "  [0, 1, 5, 2, 4, 6, 3])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_list = [\"sentence 1\", \"sentence 2\", \"sentence 3\"]\n",
    "gen_list = [\"sentence a\", \"sentence b\", \"foo bar\"]\n",
    "\n",
    "match_batch(ref_list, gen_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
