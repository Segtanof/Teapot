2025/03/19 14:49:26 routes.go:1187: INFO server config env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL:0 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:1h0m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ma/ma_ma/ma_ssiu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:8 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:0 http_proxy: https_proxy: no_proxy:]"
time=2025-03-19T14:49:26.715+01:00 level=INFO source=images.go:432 msg="total blobs: 16"
time=2025-03-19T14:49:26.716+01:00 level=INFO source=images.go:439 msg="total unused blobs removed: 0"
time=2025-03-19T14:49:26.718+01:00 level=INFO source=routes.go:1238 msg="Listening on 127.0.0.1:11434 (version 0.5.7)"
time=2025-03-19T14:49:26.719+01:00 level=INFO source=routes.go:1267 msg="Dynamic LLM libraries" runners="[cpu cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx]"
time=2025-03-19T14:49:26.719+01:00 level=INFO source=gpu.go:226 msg="looking for compatible GPUs"
time=2025-03-19T14:49:27.382+01:00 level=INFO source=types.go:131 msg="inference compute" id=GPU-4cab03b9-00af-e39d-8060-b42bce84f343 library=cuda variant=v12 compute=9.0 driver=12.4 name="NVIDIA H100 PCIe" total="79.1 GiB" available="78.6 GiB"
2025-03-19 14:49:45,548 - Script started
2025-03-19 14:49:45,548 - Processing model: llama3.2
2025-03-19 14:49:45,548 - Warming up llama3.2
time=2025-03-19T14:49:46.056+01:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-4cab03b9-00af-e39d-8060-b42bce84f343 parallel=8 available=84422623232 required="5.0 GiB"
time=2025-03-19T14:49:46.319+01:00 level=INFO source=server.go:104 msg="system memory" total="503.5 GiB" free="482.8 GiB" free_swap="14.8 GiB"
time=2025-03-19T14:49:46.320+01:00 level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[78.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.0 GiB" memory.required.partial="5.0 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="3.3 GiB" memory.weights.repeating="3.0 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="824.0 MiB" memory.graph.partial="881.1 MiB"
time=2025-03-19T14:49:46.322+01:00 level=INFO source=server.go:376 msg="starting llama server" cmd="/pfs/data5/software_uc2/bwhpc/common/cs/ollama/0.5.7/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 16384 --batch-size 512 --n-gpu-layers 29 --threads 64 --parallel 8 --port 40999"
time=2025-03-19T14:49:46.370+01:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-03-19T14:49:46.370+01:00 level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
time=2025-03-19T14:49:46.370+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
time=2025-03-19T14:49:46.921+01:00 level=INFO source=runner.go:936 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 PCIe, compute capability 9.0, VMM: yes
time=2025-03-19T14:49:47.051+01:00 level=INFO source=runner.go:937 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=64
time=2025-03-19T14:49:47.051+01:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:40999"
time=2025-03-19T14:49:47.123+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA H100 PCIe) - 80511 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 3072
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 24
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 3
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.21 B
llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) 
llm_load_print_meta: general.name     = Llama 3.2 3B Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 28 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 29/29 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_new_context_with_model: n_seq_max     = 8
llama_new_context_with_model: n_ctx         = 16384
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 4096
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 500000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1792.00 MiB
llama_new_context_with_model: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     4.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   824.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    38.01 MiB
llama_new_context_with_model: graph nodes  = 902
llama_new_context_with_model: graph splits = 2
time=2025-03-19T14:49:50.885+01:00 level=INFO source=server.go:594 msg="llama runner started in 4.51 seconds"
2025-03-19 14:49:53,412 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
llama3.2-no_prompt-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:49:54,238 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:49:54,368 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:49:54,370 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:49:54,371 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:49:54,372 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:49:54,373 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:02,447 - Batch inference for 3 titles, duration: 0:00:08.349866
2025-03-19 14:50:06,295 - Batch inference for 8 titles, duration: 0:00:12.198560
2025-03-19 14:50:06,597 - Batch inference for 8 titles, duration: 0:00:12.500879
2025-03-19 14:50:08,842 - Batch inference for 8 titles, duration: 0:00:14.746027
llama3.2-no_prompt-0:  17%|█▋        | 1/6 [00:14<01:13, 14.74s/it]2025-03-19 14:50:10,402 - Batch inference for 8 titles, duration: 0:00:16.305362
2025-03-19 14:50:10,573 - Batch inference for 8 titles, duration: 0:00:16.477012
llama3.2-no_prompt-0:  33%|███▎      | 2/6 [00:16<00:28,  7.09s/it]llama3.2-no_prompt-0: 100%|██████████| 6/6 [00:16<00:00,  2.75s/it]
2025-03-19 14:50:10,574 - ThreadPoolExecutor for llama3.2-no_prompt-0, duration: 0:00:16.478266
2025-03-19 14:50:10,584 - Assigned results for llama3.2-no_prompt-0, duration: 0:00:00.010100
2025-03-19 14:50:10,586 - Wrote result JSON for llama3.2-no_prompt-0, duration: 0:00:00.002157
llama3.2-no_prompt-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:50:10,624 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:10,642 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:10,644 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:10,645 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:10,646 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:10,673 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:21,686 - Batch inference for 8 titles, duration: 0:00:11.098757
2025-03-19 14:50:24,591 - Batch inference for 8 titles, duration: 0:00:14.003700
2025-03-19 14:50:25,640 - Batch inference for 8 titles, duration: 0:00:15.053630
llama3.2-no_prompt-1:  17%|█▋        | 1/6 [00:15<01:15, 15.05s/it]2025-03-19 14:50:25,672 - Batch inference for 3 titles, duration: 0:00:15.084782
2025-03-19 14:50:27,030 - Batch inference for 8 titles, duration: 0:00:16.443119
2025-03-19 14:50:27,176 - Batch inference for 8 titles, duration: 0:00:16.588970
llama3.2-no_prompt-1:  33%|███▎      | 2/6 [00:16<00:28,  7.10s/it]llama3.2-no_prompt-1: 100%|██████████| 6/6 [00:16<00:00,  2.76s/it]
2025-03-19 14:50:27,177 - ThreadPoolExecutor for llama3.2-no_prompt-1, duration: 0:00:16.590026
2025-03-19 14:50:27,186 - Assigned results for llama3.2-no_prompt-1, duration: 0:00:00.009597
2025-03-19 14:50:27,189 - Wrote result JSON for llama3.2-no_prompt-1, duration: 0:00:00.002194
llama3.2-no_prompt-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:50:27,226 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:27,244 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:27,245 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:27,247 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:27,248 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:27,275 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:38,565 - Batch inference for 3 titles, duration: 0:00:11.374627
2025-03-19 14:50:42,325 - Batch inference for 8 titles, duration: 0:00:15.135223
2025-03-19 14:50:44,151 - Batch inference for 8 titles, duration: 0:00:16.961711
llama3.2-no_prompt-2:  17%|█▋        | 1/6 [00:16<01:24, 16.96s/it]2025-03-19 14:50:44,196 - Batch inference for 8 titles, duration: 0:00:17.006388
2025-03-19 14:50:44,365 - Batch inference for 8 titles, duration: 0:00:17.175085
llama3.2-no_prompt-2:  50%|█████     | 3/6 [00:17<00:13,  4.48s/it]2025-03-19 14:50:44,386 - Batch inference for 8 titles, duration: 0:00:17.196152
llama3.2-no_prompt-2: 100%|██████████| 6/6 [00:17<00:00,  2.87s/it]
2025-03-19 14:50:44,386 - ThreadPoolExecutor for llama3.2-no_prompt-2, duration: 0:00:17.197334
2025-03-19 14:50:44,396 - Assigned results for llama3.2-no_prompt-2, duration: 0:00:00.009436
2025-03-19 14:50:44,398 - Wrote result JSON for llama3.2-no_prompt-2, duration: 0:00:00.002158
llama3.2-no_prompt-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:50:44,446 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:44,447 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:44,468 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:44,469 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:44,470 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:44,472 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:50:55,817 - Batch inference for 3 titles, duration: 0:00:11.416881
2025-03-19 14:50:57,961 - Batch inference for 8 titles, duration: 0:00:13.562020
2025-03-19 14:50:59,094 - Batch inference for 8 titles, duration: 0:00:14.694003
2025-03-19 14:50:59,617 - Batch inference for 8 titles, duration: 0:00:15.217364
2025-03-19 14:51:00,841 - Batch inference for 8 titles, duration: 0:00:16.441865
llama3.2-no_prompt-3:  17%|█▋        | 1/6 [00:16<01:22, 16.44s/it]2025-03-19 14:51:01,581 - Batch inference for 8 titles, duration: 0:00:17.181237
llama3.2-no_prompt-3:  50%|█████     | 3/6 [00:17<00:13,  4.54s/it]llama3.2-no_prompt-3: 100%|██████████| 6/6 [00:17<00:00,  2.86s/it]
2025-03-19 14:51:01,581 - ThreadPoolExecutor for llama3.2-no_prompt-3, duration: 0:00:17.182428
2025-03-19 14:51:01,591 - Assigned results for llama3.2-no_prompt-3, duration: 0:00:00.009541
2025-03-19 14:51:01,593 - Wrote result JSON for llama3.2-no_prompt-3, duration: 0:00:00.001840
llama3.2-no_prompt-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:51:01,630 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:01,649 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:01,650 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:01,652 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:01,653 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:01,680 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:10,654 - Batch inference for 3 titles, duration: 0:00:09.060008
2025-03-19 14:51:14,527 - Batch inference for 8 titles, duration: 0:00:12.933007
2025-03-19 14:51:16,639 - Batch inference for 8 titles, duration: 0:00:15.045692
2025-03-19 14:51:17,075 - Batch inference for 8 titles, duration: 0:00:15.481506
2025-03-19 14:51:17,544 - Batch inference for 8 titles, duration: 0:00:15.950294
llama3.2-no_prompt-4:  17%|█▋        | 1/6 [00:15<01:19, 15.95s/it]2025-03-19 14:51:18,235 - Batch inference for 8 titles, duration: 0:00:16.641119
llama3.2-no_prompt-4:  50%|█████     | 3/6 [00:16<00:13,  4.39s/it]llama3.2-no_prompt-4: 100%|██████████| 6/6 [00:16<00:00,  2.77s/it]
2025-03-19 14:51:18,236 - ThreadPoolExecutor for llama3.2-no_prompt-4, duration: 0:00:16.642266
2025-03-19 14:51:18,245 - Assigned results for llama3.2-no_prompt-4, duration: 0:00:00.009445
2025-03-19 14:51:18,247 - Wrote result JSON for llama3.2-no_prompt-4, duration: 0:00:00.002069
2025-03-19 14:51:18,249 - Wrote prompt prompt1, duration: 0:00:00.001089
llama3.2-prompt1-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:51:18,367 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:19,417 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:19,418 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:19,420 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:19,421 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:19,422 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:26,569 - Batch inference for 3 titles, duration: 0:00:08.318860
2025-03-19 14:51:27,036 - Batch inference for 8 titles, duration: 0:00:08.786344
2025-03-19 14:51:27,086 - Batch inference for 8 titles, duration: 0:00:08.836805
2025-03-19 14:51:28,686 - Batch inference for 8 titles, duration: 0:00:10.436464
2025-03-19 14:51:29,143 - Batch inference for 8 titles, duration: 0:00:10.893650
2025-03-19 14:51:29,344 - Batch inference for 8 titles, duration: 0:00:11.095462
llama3.2-prompt1-0:  17%|█▋        | 1/6 [00:11<00:55, 11.09s/it]llama3.2-prompt1-0: 100%|██████████| 6/6 [00:11<00:00,  1.85s/it]
2025-03-19 14:51:29,345 - ThreadPoolExecutor for llama3.2-prompt1-0, duration: 0:00:11.096237
2025-03-19 14:51:29,355 - Assigned results for llama3.2-prompt1-0, duration: 0:00:00.009531
2025-03-19 14:51:29,365 - Wrote result JSON for llama3.2-prompt1-0, duration: 0:00:00.010087
llama3.2-prompt1-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:51:29,413 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:29,414 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:29,438 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:29,439 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:29,441 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:29,442 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:36,894 - Batch inference for 8 titles, duration: 0:00:07.527829
2025-03-19 14:51:38,131 - Batch inference for 3 titles, duration: 0:00:08.765263
2025-03-19 14:51:39,306 - Batch inference for 8 titles, duration: 0:00:09.940197
2025-03-19 14:51:39,838 - Batch inference for 8 titles, duration: 0:00:10.471984
2025-03-19 14:51:40,306 - Batch inference for 8 titles, duration: 0:00:10.939837
2025-03-19 14:51:40,409 - Batch inference for 8 titles, duration: 0:00:11.043878
llama3.2-prompt1-1:  17%|█▋        | 1/6 [00:11<00:55, 11.04s/it]llama3.2-prompt1-1: 100%|██████████| 6/6 [00:11<00:00,  1.84s/it]
2025-03-19 14:51:40,410 - ThreadPoolExecutor for llama3.2-prompt1-1, duration: 0:00:11.044677
2025-03-19 14:51:40,419 - Assigned results for llama3.2-prompt1-1, duration: 0:00:00.009393
2025-03-19 14:51:40,421 - Wrote result JSON for llama3.2-prompt1-1, duration: 0:00:00.001873
llama3.2-prompt1-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:51:40,467 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:40,486 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:40,487 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:40,489 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:40,490 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:40,491 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:46,635 - Batch inference for 3 titles, duration: 0:00:06.212163
2025-03-19 14:51:48,338 - Batch inference for 8 titles, duration: 0:00:07.915563
2025-03-19 14:51:49,204 - Batch inference for 8 titles, duration: 0:00:08.781269
2025-03-19 14:51:51,470 - Batch inference for 8 titles, duration: 0:00:11.047594
2025-03-19 14:51:51,554 - Batch inference for 8 titles, duration: 0:00:11.132381
llama3.2-prompt1-2:  17%|█▋        | 1/6 [00:11<00:55, 11.13s/it]2025-03-19 14:51:51,745 - Batch inference for 8 titles, duration: 0:00:11.323365
llama3.2-prompt1-2:  33%|███▎      | 2/6 [00:11<00:18,  4.70s/it]llama3.2-prompt1-2: 100%|██████████| 6/6 [00:11<00:00,  1.89s/it]
2025-03-19 14:51:51,746 - ThreadPoolExecutor for llama3.2-prompt1-2, duration: 0:00:11.324423
2025-03-19 14:51:51,756 - Assigned results for llama3.2-prompt1-2, duration: 0:00:00.009444
2025-03-19 14:51:51,758 - Wrote result JSON for llama3.2-prompt1-2, duration: 0:00:00.001614
llama3.2-prompt1-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:51:51,804 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:51,805 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:51,828 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:51,830 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:51,830 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:51,832 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:51:59,076 - Batch inference for 8 titles, duration: 0:00:07.317237
2025-03-19 14:51:59,365 - Batch inference for 3 titles, duration: 0:00:07.606349
2025-03-19 14:52:00,782 - Batch inference for 8 titles, duration: 0:00:09.023862
2025-03-19 14:52:01,554 - Batch inference for 8 titles, duration: 0:00:09.796375
llama3.2-prompt1-3:  17%|█▋        | 1/6 [00:09<00:48,  9.80s/it]2025-03-19 14:52:02,462 - Batch inference for 8 titles, duration: 0:00:10.703411
llama3.2-prompt1-3:  33%|███▎      | 2/6 [00:10<00:18,  4.57s/it]2025-03-19 14:52:03,210 - Batch inference for 8 titles, duration: 0:00:11.452016
llama3.2-prompt1-3:  50%|█████     | 3/6 [00:11<00:08,  2.82s/it]llama3.2-prompt1-3: 100%|██████████| 6/6 [00:11<00:00,  1.91s/it]
2025-03-19 14:52:03,211 - ThreadPoolExecutor for llama3.2-prompt1-3, duration: 0:00:11.453317
2025-03-19 14:52:03,221 - Assigned results for llama3.2-prompt1-3, duration: 0:00:00.009422
2025-03-19 14:52:03,223 - Wrote result JSON for llama3.2-prompt1-3, duration: 0:00:00.001915
llama3.2-prompt1-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:52:03,262 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:03,280 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:03,281 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:03,282 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:03,284 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:03,314 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:10,701 - Batch inference for 8 titles, duration: 0:00:07.477424
2025-03-19 14:52:11,914 - Batch inference for 3 titles, duration: 0:00:08.690246
2025-03-19 14:52:11,992 - Batch inference for 8 titles, duration: 0:00:08.767896
2025-03-19 14:52:13,517 - Batch inference for 8 titles, duration: 0:00:10.293735
2025-03-19 14:52:13,885 - Batch inference for 8 titles, duration: 0:00:10.661855
llama3.2-prompt1-4:  17%|█▋        | 1/6 [00:10<00:53, 10.66s/it]2025-03-19 14:52:14,001 - Batch inference for 8 titles, duration: 0:00:10.777938
llama3.2-prompt1-4:  33%|███▎      | 2/6 [00:10<00:17,  4.46s/it]llama3.2-prompt1-4: 100%|██████████| 6/6 [00:10<00:00,  1.80s/it]
2025-03-19 14:52:14,002 - ThreadPoolExecutor for llama3.2-prompt1-4, duration: 0:00:10.778919
2025-03-19 14:52:14,012 - Assigned results for llama3.2-prompt1-4, duration: 0:00:00.009744
2025-03-19 14:52:14,014 - Wrote result JSON for llama3.2-prompt1-4, duration: 0:00:00.001681
2025-03-19 14:52:14,014 - Processing model: mistral
2025-03-19 14:52:14,014 - Warming up mistral
time=2025-03-19T14:52:14.465+01:00 level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-4cab03b9-00af-e39d-8060-b42bce84f343 library=cuda total="79.1 GiB" available="73.4 GiB"
time=2025-03-19T14:52:14.465+01:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 gpu=GPU-4cab03b9-00af-e39d-8060-b42bce84f343 parallel=8 available=78857240576 required="7.4 GiB"
time=2025-03-19T14:52:14.732+01:00 level=INFO source=server.go:104 msg="system memory" total="503.5 GiB" free="481.9 GiB" free_swap="14.8 GiB"
time=2025-03-19T14:52:14.733+01:00 level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[73.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="7.4 GiB" memory.required.partial="7.4 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[7.4 GiB]" memory.weights.total="5.7 GiB" memory.weights.repeating="5.6 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.1 GiB"
time=2025-03-19T14:52:14.735+01:00 level=INFO source=server.go:376 msg="starting llama server" cmd="/pfs/data5/software_uc2/bwhpc/common/cs/ollama/0.5.7/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 16384 --batch-size 512 --n-gpu-layers 33 --threads 64 --parallel 8 --port 33655"
time=2025-03-19T14:52:14.735+01:00 level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-03-19T14:52:14.735+01:00 level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
time=2025-03-19T14:52:14.735+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
time=2025-03-19T14:52:14.768+01:00 level=INFO source=runner.go:936 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 PCIe, compute capability 9.0, VMM: yes
time=2025-03-19T14:52:14.812+01:00 level=INFO source=runner.go:937 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=64
time=2025-03-19T14:52:14.812+01:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:33655"
time=2025-03-19T14:52:14.986+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA H100 PCIe) - 75204 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 771
llm_load_vocab: token to piece cache size = 0.1731 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32768
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.25 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = Mistral-7B-Instruct-v0.3
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 781 '<0x0A>'
llm_load_print_meta: EOG token        = 2 '</s>'
llm_load_print_meta: max token length = 48
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =  3850.02 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    72.00 MiB
llama_new_context_with_model: n_seq_max     = 8
llama_new_context_with_model: n_ctx         = 16384
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 4096
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     1.12 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB
llama_new_context_with_model: graph nodes  = 1030
llama_new_context_with_model: graph splits = 2
time=2025-03-19T14:52:21.253+01:00 level=INFO source=server.go:594 msg="llama runner started in 6.52 seconds"
2025-03-19 14:52:21,840 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
mistral-no_prompt-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:52:27,755 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:27,829 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:27,829 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:27,829 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:27,830 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:27,831 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:35,177 - Batch inference for 3 titles, duration: 0:00:07.456878
2025-03-19 14:52:38,811 - Batch inference for 8 titles, duration: 0:00:11.091061
2025-03-19 14:52:39,655 - Batch inference for 8 titles, duration: 0:00:11.935433
2025-03-19 14:52:40,209 - Batch inference for 8 titles, duration: 0:00:12.489188
2025-03-19 14:52:40,467 - Batch inference for 8 titles, duration: 0:00:12.747168
2025-03-19 14:52:40,764 - Batch inference for 8 titles, duration: 0:00:13.044465
mistral-no_prompt-0:  17%|█▋        | 1/6 [00:13<01:05, 13.04s/it]mistral-no_prompt-0: 100%|██████████| 6/6 [00:13<00:00,  2.17s/it]
2025-03-19 14:52:40,764 - ThreadPoolExecutor for mistral-no_prompt-0, duration: 0:00:13.045289
2025-03-19 14:52:40,774 - Assigned results for mistral-no_prompt-0, duration: 0:00:00.009422
2025-03-19 14:52:40,776 - Wrote result JSON for mistral-no_prompt-0, duration: 0:00:00.001501
mistral-no_prompt-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:52:40,797 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:40,815 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:40,815 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:40,815 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:40,816 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:40,817 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:51,697 - Batch inference for 8 titles, duration: 0:00:10.920593
2025-03-19 14:52:53,122 - Batch inference for 8 titles, duration: 0:00:12.345580
2025-03-19 14:52:53,222 - Batch inference for 3 titles, duration: 0:00:12.445413
2025-03-19 14:52:53,355 - Batch inference for 8 titles, duration: 0:00:12.578418
2025-03-19 14:52:53,438 - Batch inference for 8 titles, duration: 0:00:12.661297
2025-03-19 14:52:55,948 - Batch inference for 8 titles, duration: 0:00:15.171746
mistral-no_prompt-1:  17%|█▋        | 1/6 [00:15<01:15, 15.17s/it]mistral-no_prompt-1: 100%|██████████| 6/6 [00:15<00:00,  2.53s/it]
2025-03-19 14:52:55,949 - ThreadPoolExecutor for mistral-no_prompt-1, duration: 0:00:15.172649
2025-03-19 14:52:55,958 - Assigned results for mistral-no_prompt-1, duration: 0:00:00.009561
2025-03-19 14:52:55,960 - Wrote result JSON for mistral-no_prompt-1, duration: 0:00:00.001856
mistral-no_prompt-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:52:55,983 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:56,001 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:56,001 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:56,002 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:56,002 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:52:56,003 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:03,440 - Batch inference for 8 titles, duration: 0:00:07.479739
mistral-no_prompt-2:  17%|█▋        | 1/6 [00:07<00:37,  7.48s/it]2025-03-19 14:53:06,952 - Batch inference for 8 titles, duration: 0:00:10.991573
mistral-no_prompt-2:  33%|███▎      | 2/6 [00:10<00:20,  5.15s/it]2025-03-19 14:53:06,974 - Batch inference for 3 titles, duration: 0:00:11.012569
2025-03-19 14:53:07,084 - Batch inference for 8 titles, duration: 0:00:11.122694
2025-03-19 14:53:08,069 - Batch inference for 8 titles, duration: 0:00:12.107682
mistral-no_prompt-2:  50%|█████     | 3/6 [00:12<00:09,  3.31s/it]2025-03-19 14:53:08,848 - Batch inference for 8 titles, duration: 0:00:12.886755
mistral-no_prompt-2:  83%|████████▎ | 5/6 [00:12<00:01,  1.65s/it]mistral-no_prompt-2: 100%|██████████| 6/6 [00:12<00:00,  2.15s/it]
2025-03-19 14:53:08,849 - ThreadPoolExecutor for mistral-no_prompt-2, duration: 0:00:12.888173
2025-03-19 14:53:08,858 - Assigned results for mistral-no_prompt-2, duration: 0:00:00.009450
2025-03-19 14:53:08,860 - Wrote result JSON for mistral-no_prompt-2, duration: 0:00:00.001569
mistral-no_prompt-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:53:08,884 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:08,901 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:08,902 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:08,902 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:08,902 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:08,903 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:18,309 - Batch inference for 8 titles, duration: 0:00:09.448177
2025-03-19 14:53:22,023 - Batch inference for 8 titles, duration: 0:00:13.162444
2025-03-19 14:53:22,283 - Batch inference for 8 titles, duration: 0:00:13.422470
2025-03-19 14:53:22,705 - Batch inference for 8 titles, duration: 0:00:13.844332
2025-03-19 14:53:22,837 - Batch inference for 8 titles, duration: 0:00:13.976244
mistral-no_prompt-3:  17%|█▋        | 1/6 [00:13<01:09, 13.98s/it]2025-03-19 14:53:24,129 - Batch inference for 3 titles, duration: 0:00:15.268275
mistral-no_prompt-3: 100%|██████████| 6/6 [00:15<00:00,  1.94s/it]mistral-no_prompt-3: 100%|██████████| 6/6 [00:15<00:00,  2.54s/it]
2025-03-19 14:53:24,130 - ThreadPoolExecutor for mistral-no_prompt-3, duration: 0:00:15.269984
2025-03-19 14:53:24,140 - Assigned results for mistral-no_prompt-3, duration: 0:00:00.009480
2025-03-19 14:53:24,142 - Wrote result JSON for mistral-no_prompt-3, duration: 0:00:00.001837
mistral-no_prompt-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:53:24,171 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:24,171 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:24,171 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:24,195 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:24,196 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:24,196 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:35,464 - Batch inference for 8 titles, duration: 0:00:11.321159
2025-03-19 14:53:36,387 - Batch inference for 8 titles, duration: 0:00:12.244281
2025-03-19 14:53:36,716 - Batch inference for 8 titles, duration: 0:00:12.573489
2025-03-19 14:53:37,243 - Batch inference for 3 titles, duration: 0:00:13.099458
2025-03-19 14:53:39,089 - Batch inference for 8 titles, duration: 0:00:14.946338
2025-03-19 14:53:40,450 - Batch inference for 8 titles, duration: 0:00:16.307438
mistral-no_prompt-4:  17%|█▋        | 1/6 [00:16<01:21, 16.31s/it]mistral-no_prompt-4: 100%|██████████| 6/6 [00:16<00:00,  2.72s/it]
2025-03-19 14:53:40,450 - ThreadPoolExecutor for mistral-no_prompt-4, duration: 0:00:16.308276
2025-03-19 14:53:40,460 - Assigned results for mistral-no_prompt-4, duration: 0:00:00.009398
2025-03-19 14:53:40,462 - Wrote result JSON for mistral-no_prompt-4, duration: 0:00:00.001802
2025-03-19 14:53:40,462 - Wrote prompt prompt1, duration: 0:00:00.000227
mistral-prompt1-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:53:40,541 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:41,206 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:41,207 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:41,207 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:41,207 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:41,207 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:45,243 - Batch inference for 3 titles, duration: 0:00:04.779696
2025-03-19 14:53:46,843 - Batch inference for 8 titles, duration: 0:00:06.380682
mistral-prompt1-0:  17%|█▋        | 1/6 [00:06<00:31,  6.38s/it]2025-03-19 14:53:48,255 - Batch inference for 8 titles, duration: 0:00:07.791835
mistral-prompt1-0:  33%|███▎      | 2/6 [00:07<00:13,  3.46s/it]2025-03-19 14:53:49,241 - Batch inference for 8 titles, duration: 0:00:08.777617
2025-03-19 14:53:51,532 - Batch inference for 8 titles, duration: 0:00:11.068482
2025-03-19 14:53:51,796 - Batch inference for 8 titles, duration: 0:00:11.332492
mistral-prompt1-0:  50%|█████     | 3/6 [00:11<00:10,  3.50s/it]mistral-prompt1-0: 100%|██████████| 6/6 [00:11<00:00,  1.89s/it]
2025-03-19 14:53:51,796 - ThreadPoolExecutor for mistral-prompt1-0, duration: 0:00:11.333739
2025-03-19 14:53:51,806 - Assigned results for mistral-prompt1-0, duration: 0:00:00.009393
2025-03-19 14:53:51,808 - Wrote result JSON for mistral-prompt1-0, duration: 0:00:00.001850
mistral-prompt1-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:53:51,831 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:51,851 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:51,851 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:51,852 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:51,852 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:51,852 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:53:55,862 - Batch inference for 3 titles, duration: 0:00:04.052587
2025-03-19 14:53:57,956 - Batch inference for 8 titles, duration: 0:00:06.147313
2025-03-19 14:53:58,024 - Batch inference for 8 titles, duration: 0:00:06.215402
mistral-prompt1-1:  17%|█▋        | 1/6 [00:06<00:31,  6.21s/it]2025-03-19 14:53:59,120 - Batch inference for 8 titles, duration: 0:00:07.311601
2025-03-19 14:54:00,682 - Batch inference for 8 titles, duration: 0:00:08.872737
2025-03-19 14:54:01,402 - Batch inference for 8 titles, duration: 0:00:09.592892
mistral-prompt1-1:  50%|█████     | 3/6 [00:09<00:08,  2.86s/it]mistral-prompt1-1: 100%|██████████| 6/6 [00:09<00:00,  1.60s/it]
2025-03-19 14:54:01,402 - ThreadPoolExecutor for mistral-prompt1-1, duration: 0:00:09.594169
2025-03-19 14:54:01,412 - Assigned results for mistral-prompt1-1, duration: 0:00:00.009417
2025-03-19 14:54:01,414 - Wrote result JSON for mistral-prompt1-1, duration: 0:00:00.001604
mistral-prompt1-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:54:01,438 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:01,458 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:01,458 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:01,458 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:01,459 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:01,459 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:08,106 - Batch inference for 3 titles, duration: 0:00:06.691682
2025-03-19 14:54:08,390 - Batch inference for 8 titles, duration: 0:00:06.975684
2025-03-19 14:54:09,586 - Batch inference for 8 titles, duration: 0:00:08.171726
2025-03-19 14:54:10,080 - Batch inference for 8 titles, duration: 0:00:08.664910
2025-03-19 14:54:11,220 - Batch inference for 8 titles, duration: 0:00:09.805430
2025-03-19 14:54:11,711 - Batch inference for 8 titles, duration: 0:00:10.297005
mistral-prompt1-2:  17%|█▋        | 1/6 [00:10<00:51, 10.30s/it]mistral-prompt1-2: 100%|██████████| 6/6 [00:10<00:00,  1.72s/it]
2025-03-19 14:54:11,712 - ThreadPoolExecutor for mistral-prompt1-2, duration: 0:00:10.297777
2025-03-19 14:54:11,721 - Assigned results for mistral-prompt1-2, duration: 0:00:00.009470
2025-03-19 14:54:11,724 - Wrote result JSON for mistral-prompt1-2, duration: 0:00:00.002203
mistral-prompt1-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:54:11,750 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:11,750 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:11,773 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:11,773 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:11,773 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:11,774 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:16,447 - Batch inference for 3 titles, duration: 0:00:04.722209
2025-03-19 14:54:18,519 - Batch inference for 8 titles, duration: 0:00:06.794764
2025-03-19 14:54:20,436 - Batch inference for 8 titles, duration: 0:00:08.711528
2025-03-19 14:54:21,168 - Batch inference for 8 titles, duration: 0:00:09.443448
2025-03-19 14:54:22,132 - Batch inference for 8 titles, duration: 0:00:10.408354
mistral-prompt1-3:  17%|█▋        | 1/6 [00:10<00:52, 10.41s/it]2025-03-19 14:54:22,380 - Batch inference for 8 titles, duration: 0:00:10.655510
mistral-prompt1-3:  50%|█████     | 3/6 [00:10<00:08,  2.79s/it]mistral-prompt1-3: 100%|██████████| 6/6 [00:10<00:00,  1.78s/it]
2025-03-19 14:54:22,381 - ThreadPoolExecutor for mistral-prompt1-3, duration: 0:00:10.656763
2025-03-19 14:54:22,390 - Assigned results for mistral-prompt1-3, duration: 0:00:00.009378
2025-03-19 14:54:22,392 - Wrote result JSON for mistral-prompt1-3, duration: 0:00:00.001797
mistral-prompt1-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 14:54:22,417 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:22,437 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:22,438 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:22,439 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:22,439 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:22,439 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 14:54:27,257 - Batch inference for 3 titles, duration: 0:00:04.863872
2025-03-19 14:54:30,824 - Batch inference for 8 titles, duration: 0:00:08.431204
2025-03-19 14:54:32,069 - Batch inference for 8 titles, duration: 0:00:09.676179
mistral-prompt1-4:  17%|█▋        | 1/6 [00:09<00:48,  9.68s/it]2025-03-19 14:54:32,615 - Batch inference for 8 titles, duration: 0:00:10.221850
2025-03-19 14:54:33,226 - Batch inference for 8 titles, duration: 0:00:10.833297
mistral-prompt1-4:  50%|█████     | 3/6 [00:10<00:08,  2.94s/it]2025-03-19 14:54:33,284 - Batch inference for 8 titles, duration: 0:00:10.891021
mistral-prompt1-4: 100%|██████████| 6/6 [00:10<00:00,  1.82s/it]
2025-03-19 14:54:33,285 - ThreadPoolExecutor for mistral-prompt1-4, duration: 0:00:10.892390
2025-03-19 14:54:33,294 - Assigned results for mistral-prompt1-4, duration: 0:00:00.009351
2025-03-19 14:54:33,297 - Wrote result JSON for mistral-prompt1-4, duration: 0:00:00.002468
2025-03-19 14:54:33,297 - Processing model: qwen2.5
2025-03-19 14:54:33,297 - Warming up qwen2.5
2025-03-19 14:54:33,300 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
Traceback (most recent call last):
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/teapot/2_new_bench.py", line 219, in <module>
    model.invoke("Warm-up prompt")  # Dummy call
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 307, in invoke
    self.generate_prompt(
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 843, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 683, in generate
    self._generate_with_cache(
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 908, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_ollama/chat_models.py", line 701, in _generate
    final_chunk = self._chat_stream_with_aggregation(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_ollama/chat_models.py", line 602, in _chat_stream_with_aggregation
    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_ollama/chat_models.py", line 589, in _create_chat_stream
    yield from self._client.chat(**chat_params)
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/ollama/_client.py", line 168, in inner
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: model "qwen2.5" not found, try pulling it first (status code: 404)
