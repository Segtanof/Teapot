2025/03/19 15:00:04 routes.go:1187: INFO server config env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL:0 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:1h0m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ma/ma_ma/ma_ssiu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:8 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:0 http_proxy: https_proxy: no_proxy:]"
time=2025-03-19T15:00:04.366+01:00 level=INFO source=images.go:432 msg="total blobs: 16"
time=2025-03-19T15:00:04.367+01:00 level=INFO source=images.go:439 msg="total unused blobs removed: 0"
time=2025-03-19T15:00:04.369+01:00 level=INFO source=routes.go:1238 msg="Listening on 127.0.0.1:11434 (version 0.5.7)"
time=2025-03-19T15:00:04.370+01:00 level=INFO source=routes.go:1267 msg="Dynamic LLM libraries" runners="[cpu cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx]"
time=2025-03-19T15:00:04.370+01:00 level=INFO source=gpu.go:226 msg="looking for compatible GPUs"
time=2025-03-19T15:00:05.020+01:00 level=INFO source=types.go:131 msg="inference compute" id=GPU-c84f8fdc-c01c-c96b-dfb4-dce174caf888 library=cuda variant=v12 compute=9.0 driver=12.4 name="NVIDIA H100 PCIe" total="79.1 GiB" available="78.6 GiB"
2025-03-19 15:00:20,223 - Script started
2025-03-19 15:00:20,224 - Processing model: llama3.2
2025-03-19 15:00:20,224 - Warming up llama3.2
time=2025-03-19T15:00:20.541+01:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-c84f8fdc-c01c-c96b-dfb4-dce174caf888 parallel=8 available=84422623232 required="5.0 GiB"
time=2025-03-19T15:00:20.812+01:00 level=INFO source=server.go:104 msg="system memory" total="503.5 GiB" free="481.7 GiB" free_swap="14.8 GiB"
time=2025-03-19T15:00:20.812+01:00 level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[78.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.0 GiB" memory.required.partial="5.0 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="3.3 GiB" memory.weights.repeating="3.0 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="824.0 MiB" memory.graph.partial="881.1 MiB"
time=2025-03-19T15:00:20.815+01:00 level=INFO source=server.go:376 msg="starting llama server" cmd="/pfs/data5/software_uc2/bwhpc/common/cs/ollama/0.5.7/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 16384 --batch-size 512 --n-gpu-layers 29 --threads 64 --parallel 8 --port 36329"
time=2025-03-19T15:00:20.815+01:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-03-19T15:00:20.815+01:00 level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
time=2025-03-19T15:00:20.815+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
time=2025-03-19T15:00:20.848+01:00 level=INFO source=runner.go:936 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 PCIe, compute capability 9.0, VMM: yes
time=2025-03-19T15:00:20.891+01:00 level=INFO source=runner.go:937 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=64
time=2025-03-19T15:00:20.891+01:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:36329"
time=2025-03-19T15:00:21.067+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA H100 PCIe) - 80511 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 3072
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 24
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 3
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.21 B
llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) 
llm_load_print_meta: general.name     = Llama 3.2 3B Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 28 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 29/29 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_new_context_with_model: n_seq_max     = 8
llama_new_context_with_model: n_ctx         = 16384
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 4096
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 500000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1792.00 MiB
llama_new_context_with_model: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     4.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   824.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    38.01 MiB
llama_new_context_with_model: graph nodes  = 902
llama_new_context_with_model: graph splits = 2
time=2025-03-19T15:00:22.321+01:00 level=INFO source=server.go:594 msg="llama runner started in 1.51 seconds"
2025-03-19 15:00:22,411 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
llama3.2-no_prompt-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:00:23,006 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:23,058 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:23,059 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:23,060 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:23,062 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:23,063 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:32,157 - Batch inference for 3 titles, duration: 0:00:09.194936
2025-03-19 15:00:39,460 - Batch inference for 8 titles, duration: 0:00:16.498882
llama3.2-no_prompt-0:  17%|█▋        | 1/6 [00:16<01:22, 16.50s/it]2025-03-19 15:00:40,745 - Batch inference for 8 titles, duration: 0:00:17.783610
llama3.2-no_prompt-0:  33%|███▎      | 2/6 [00:17<00:30,  7.55s/it]2025-03-19 15:00:40,855 - Batch inference for 8 titles, duration: 0:00:17.893496
llama3.2-no_prompt-0:  50%|█████     | 3/6 [00:17<00:12,  4.15s/it]2025-03-19 15:00:41,938 - Batch inference for 8 titles, duration: 0:00:18.977125
llama3.2-no_prompt-0:  67%|██████▋   | 4/6 [00:18<00:05,  2.94s/it]2025-03-19 15:00:43,575 - Batch inference for 8 titles, duration: 0:00:20.613905
llama3.2-no_prompt-0:  83%|████████▎ | 5/6 [00:20<00:02,  2.47s/it]llama3.2-no_prompt-0: 100%|██████████| 6/6 [00:20<00:00,  3.44s/it]
2025-03-19 15:00:43,576 - ThreadPoolExecutor for llama3.2-no_prompt-0, duration: 0:00:20.615615
2025-03-19 15:00:43,586 - Assigned results for llama3.2-no_prompt-0, duration: 0:00:00.010028
2025-03-19 15:00:43,588 - Wrote result JSON for llama3.2-no_prompt-0, duration: 0:00:00.002057
llama3.2-no_prompt-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:00:43,631 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:43,649 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:43,650 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:43,651 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:43,652 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:43,654 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:00:54,110 - Batch inference for 3 titles, duration: 0:00:10.520467
2025-03-19 15:00:54,187 - Batch inference for 8 titles, duration: 0:00:10.597472
2025-03-19 15:00:57,317 - Batch inference for 8 titles, duration: 0:00:13.727154
2025-03-19 15:00:58,369 - Batch inference for 8 titles, duration: 0:00:14.780076
llama3.2-no_prompt-1:  17%|█▋        | 1/6 [00:14<01:13, 14.78s/it]2025-03-19 15:00:59,466 - Batch inference for 8 titles, duration: 0:00:15.876844
llama3.2-no_prompt-1:  33%|███▎      | 2/6 [00:15<00:26,  6.73s/it]2025-03-19 15:01:02,229 - Batch inference for 8 titles, duration: 0:00:18.639438
llama3.2-no_prompt-1:  50%|█████     | 3/6 [00:18<00:14,  4.92s/it]llama3.2-no_prompt-1: 100%|██████████| 6/6 [00:18<00:00,  3.11s/it]
2025-03-19 15:01:02,229 - ThreadPoolExecutor for llama3.2-no_prompt-1, duration: 0:00:18.640711
2025-03-19 15:01:02,239 - Assigned results for llama3.2-no_prompt-1, duration: 0:00:00.009679
2025-03-19 15:01:02,242 - Wrote result JSON for llama3.2-no_prompt-1, duration: 0:00:00.002606
llama3.2-no_prompt-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:01:02,283 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:02,300 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:02,301 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:02,302 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:02,304 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:02,305 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:11,724 - Batch inference for 3 titles, duration: 0:00:09.480497
2025-03-19 15:01:15,917 - Batch inference for 8 titles, duration: 0:00:13.673484
2025-03-19 15:01:17,444 - Batch inference for 8 titles, duration: 0:00:15.201834
llama3.2-no_prompt-2:  17%|█▋        | 1/6 [00:15<01:16, 15.20s/it]2025-03-19 15:01:17,498 - Batch inference for 8 titles, duration: 0:00:15.254948
2025-03-19 15:01:17,870 - Batch inference for 8 titles, duration: 0:00:15.627384
2025-03-19 15:01:17,949 - Batch inference for 8 titles, duration: 0:00:15.706778
llama3.2-no_prompt-2:  33%|███▎      | 2/6 [00:15<00:26,  6.56s/it]llama3.2-no_prompt-2: 100%|██████████| 6/6 [00:15<00:00,  2.62s/it]
2025-03-19 15:01:17,950 - ThreadPoolExecutor for llama3.2-no_prompt-2, duration: 0:00:15.707817
2025-03-19 15:01:17,960 - Assigned results for llama3.2-no_prompt-2, duration: 0:00:00.009514
2025-03-19 15:01:17,962 - Wrote result JSON for llama3.2-no_prompt-2, duration: 0:00:00.001669
llama3.2-no_prompt-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:01:18,005 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:18,022 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:18,024 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:18,025 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:18,026 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:18,028 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:29,113 - Batch inference for 8 titles, duration: 0:00:11.150671
2025-03-19 15:01:30,277 - Batch inference for 3 titles, duration: 0:00:12.314277
2025-03-19 15:01:33,437 - Batch inference for 8 titles, duration: 0:00:15.474674
2025-03-19 15:01:34,547 - Batch inference for 8 titles, duration: 0:00:16.584884
2025-03-19 15:01:34,695 - Batch inference for 8 titles, duration: 0:00:16.732978
2025-03-19 15:01:36,653 - Batch inference for 8 titles, duration: 0:00:18.691463
llama3.2-no_prompt-3:  17%|█▋        | 1/6 [00:18<01:33, 18.69s/it]llama3.2-no_prompt-3: 100%|██████████| 6/6 [00:18<00:00,  3.12s/it]
2025-03-19 15:01:36,654 - ThreadPoolExecutor for llama3.2-no_prompt-3, duration: 0:00:18.692284
2025-03-19 15:01:36,664 - Assigned results for llama3.2-no_prompt-3, duration: 0:00:00.009565
2025-03-19 15:01:36,668 - Wrote result JSON for llama3.2-no_prompt-3, duration: 0:00:00.003620
llama3.2-no_prompt-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:01:36,712 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:36,729 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:36,730 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:36,732 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:36,733 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:36,734 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:43,457 - Batch inference for 3 titles, duration: 0:00:06.788679
2025-03-19 15:01:49,151 - Batch inference for 8 titles, duration: 0:00:12.482968
2025-03-19 15:01:51,761 - Batch inference for 8 titles, duration: 0:00:15.092907
llama3.2-no_prompt-4:  17%|█▋        | 1/6 [00:15<01:15, 15.09s/it]2025-03-19 15:01:52,952 - Batch inference for 8 titles, duration: 0:00:16.283690
llama3.2-no_prompt-4:  33%|███▎      | 2/6 [00:16<00:27,  6.91s/it]2025-03-19 15:01:53,575 - Batch inference for 8 titles, duration: 0:00:16.906409
2025-03-19 15:01:55,371 - Batch inference for 8 titles, duration: 0:00:18.702514
llama3.2-no_prompt-4:  50%|█████     | 3/6 [00:18<00:14,  4.86s/it]llama3.2-no_prompt-4: 100%|██████████| 6/6 [00:18<00:00,  3.12s/it]
2025-03-19 15:01:55,372 - ThreadPoolExecutor for llama3.2-no_prompt-4, duration: 0:00:18.703666
2025-03-19 15:01:55,381 - Assigned results for llama3.2-no_prompt-4, duration: 0:00:00.009465
2025-03-19 15:01:55,383 - Wrote result JSON for llama3.2-no_prompt-4, duration: 0:00:00.002180
llama3.2-no_prompt-5:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:01:55,420 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:55,438 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:55,439 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:55,440 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:55,468 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:01:55,469 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:05,568 - Batch inference for 3 titles, duration: 0:00:10.183594
2025-03-19 15:02:07,495 - Batch inference for 8 titles, duration: 0:00:12.110505
2025-03-19 15:02:07,742 - Batch inference for 8 titles, duration: 0:00:12.357845
2025-03-19 15:02:10,568 - Batch inference for 8 titles, duration: 0:00:15.184114
llama3.2-no_prompt-5:  17%|█▋        | 1/6 [00:15<01:15, 15.18s/it]2025-03-19 15:02:11,518 - Batch inference for 8 titles, duration: 0:00:16.133825
2025-03-19 15:02:12,487 - Batch inference for 8 titles, duration: 0:00:17.103008
llama3.2-no_prompt-5:  50%|█████     | 3/6 [00:17<00:13,  4.65s/it]llama3.2-no_prompt-5: 100%|██████████| 6/6 [00:17<00:00,  2.85s/it]
2025-03-19 15:02:12,488 - ThreadPoolExecutor for llama3.2-no_prompt-5, duration: 0:00:17.104138
2025-03-19 15:02:12,498 - Assigned results for llama3.2-no_prompt-5, duration: 0:00:00.009572
2025-03-19 15:02:12,501 - Wrote result JSON for llama3.2-no_prompt-5, duration: 0:00:00.003360
llama3.2-no_prompt-6:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:02:12,539 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:12,557 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:12,558 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:12,559 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:12,561 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:12,588 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:22,427 - Batch inference for 8 titles, duration: 0:00:09.924744
2025-03-19 15:02:23,259 - Batch inference for 8 titles, duration: 0:00:10.756692
2025-03-19 15:02:24,252 - Batch inference for 3 titles, duration: 0:00:11.749410
2025-03-19 15:02:25,313 - Batch inference for 8 titles, duration: 0:00:12.811548
2025-03-19 15:02:28,285 - Batch inference for 8 titles, duration: 0:00:15.783370
llama3.2-no_prompt-6:  17%|█▋        | 1/6 [00:15<01:18, 15.78s/it]2025-03-19 15:02:31,719 - Batch inference for 8 titles, duration: 0:00:19.216960
llama3.2-no_prompt-6:  50%|█████     | 3/6 [00:19<00:16,  5.36s/it]llama3.2-no_prompt-6: 100%|██████████| 6/6 [00:19<00:00,  3.20s/it]
2025-03-19 15:02:31,719 - ThreadPoolExecutor for llama3.2-no_prompt-6, duration: 0:00:19.218142
2025-03-19 15:02:31,729 - Assigned results for llama3.2-no_prompt-6, duration: 0:00:00.009489
2025-03-19 15:02:31,733 - Wrote result JSON for llama3.2-no_prompt-6, duration: 0:00:00.003292
llama3.2-no_prompt-7:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:02:31,769 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:31,787 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:31,789 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:31,790 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:31,815 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:31,816 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:41,392 - Batch inference for 3 titles, duration: 0:00:09.658172
2025-03-19 15:02:44,421 - Batch inference for 8 titles, duration: 0:00:12.687783
2025-03-19 15:02:47,937 - Batch inference for 8 titles, duration: 0:00:16.203485
2025-03-19 15:02:47,980 - Batch inference for 8 titles, duration: 0:00:16.246034
2025-03-19 15:02:48,959 - Batch inference for 8 titles, duration: 0:00:17.225372
2025-03-19 15:02:50,058 - Batch inference for 8 titles, duration: 0:00:18.325127
llama3.2-no_prompt-7:  17%|█▋        | 1/6 [00:18<01:31, 18.32s/it]llama3.2-no_prompt-7: 100%|██████████| 6/6 [00:18<00:00,  3.05s/it]
2025-03-19 15:02:50,059 - ThreadPoolExecutor for llama3.2-no_prompt-7, duration: 0:00:18.326037
2025-03-19 15:02:50,069 - Assigned results for llama3.2-no_prompt-7, duration: 0:00:00.009587
2025-03-19 15:02:50,073 - Wrote result JSON for llama3.2-no_prompt-7, duration: 0:00:00.003748
llama3.2-no_prompt-8:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:02:50,116 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:50,117 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:50,138 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:50,139 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:50,140 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:02:50,141 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:02,239 - Batch inference for 8 titles, duration: 0:00:12.166104
2025-03-19 15:03:02,794 - Batch inference for 8 titles, duration: 0:00:12.721269
2025-03-19 15:03:04,307 - Batch inference for 3 titles, duration: 0:00:14.233471
2025-03-19 15:03:05,191 - Batch inference for 8 titles, duration: 0:00:15.118049
2025-03-19 15:03:05,322 - Batch inference for 8 titles, duration: 0:00:15.248751
2025-03-19 15:03:05,361 - Batch inference for 8 titles, duration: 0:00:15.288485
llama3.2-no_prompt-8:  17%|█▋        | 1/6 [00:15<01:16, 15.29s/it]llama3.2-no_prompt-8: 100%|██████████| 6/6 [00:15<00:00,  2.55s/it]
2025-03-19 15:03:05,362 - ThreadPoolExecutor for llama3.2-no_prompt-8, duration: 0:00:15.289259
2025-03-19 15:03:05,372 - Assigned results for llama3.2-no_prompt-8, duration: 0:00:00.009506
2025-03-19 15:03:05,374 - Wrote result JSON for llama3.2-no_prompt-8, duration: 0:00:00.002397
llama3.2-no_prompt-9:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:03:05,411 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:05,429 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:05,430 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:05,431 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:05,433 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:05,460 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:14,805 - Batch inference for 3 titles, duration: 0:00:09.430124
2025-03-19 15:03:16,445 - Batch inference for 8 titles, duration: 0:00:11.069894
2025-03-19 15:03:19,521 - Batch inference for 8 titles, duration: 0:00:14.146484
llama3.2-no_prompt-9:  17%|█▋        | 1/6 [00:14<01:10, 14.15s/it]2025-03-19 15:03:20,073 - Batch inference for 8 titles, duration: 0:00:14.697793
llama3.2-no_prompt-9:  33%|███▎      | 2/6 [00:14<00:24,  6.15s/it]2025-03-19 15:03:20,344 - Batch inference for 8 titles, duration: 0:00:14.968515
2025-03-19 15:03:22,148 - Batch inference for 8 titles, duration: 0:00:16.772873
llama3.2-no_prompt-9:  50%|█████     | 3/6 [00:16<00:12,  4.29s/it]llama3.2-no_prompt-9: 100%|██████████| 6/6 [00:16<00:00,  2.80s/it]
2025-03-19 15:03:22,150 - ThreadPoolExecutor for llama3.2-no_prompt-9, duration: 0:00:16.775594
2025-03-19 15:03:22,160 - Assigned results for llama3.2-no_prompt-9, duration: 0:00:00.009593
2025-03-19 15:03:22,164 - Wrote result JSON for llama3.2-no_prompt-9, duration: 0:00:00.003620
2025-03-19 15:03:22,165 - Wrote prompt prompt1, duration: 0:00:00.000915
llama3.2-prompt1-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:03:22,345 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:22,346 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:22,629 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:22,630 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:22,631 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:22,633 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:30,593 - Batch inference for 3 titles, duration: 0:00:08.427176
2025-03-19 15:03:33,387 - Batch inference for 8 titles, duration: 0:00:11.221798
2025-03-19 15:03:33,921 - Batch inference for 8 titles, duration: 0:00:11.755146
2025-03-19 15:03:34,678 - Batch inference for 8 titles, duration: 0:00:12.513043
2025-03-19 15:03:35,828 - Batch inference for 8 titles, duration: 0:00:13.663131
2025-03-19 15:03:36,538 - Batch inference for 8 titles, duration: 0:00:14.373510
llama3.2-prompt1-0:  17%|█▋        | 1/6 [00:14<01:11, 14.37s/it]llama3.2-prompt1-0: 100%|██████████| 6/6 [00:14<00:00,  2.40s/it]
2025-03-19 15:03:36,540 - ThreadPoolExecutor for llama3.2-prompt1-0, duration: 0:00:14.375275
2025-03-19 15:03:36,550 - Assigned results for llama3.2-prompt1-0, duration: 0:00:00.009594
2025-03-19 15:03:36,553 - Wrote result JSON for llama3.2-prompt1-0, duration: 0:00:00.003150
llama3.2-prompt1-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:03:36,593 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:36,612 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:36,613 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:36,613 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:36,615 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:36,616 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:42,579 - Batch inference for 3 titles, duration: 0:00:06.024659
2025-03-19 15:03:47,790 - Batch inference for 8 titles, duration: 0:00:11.236365
2025-03-19 15:03:48,285 - Batch inference for 8 titles, duration: 0:00:11.731595
llama3.2-prompt1-1:  17%|█▋        | 1/6 [00:11<00:58, 11.73s/it]2025-03-19 15:03:48,395 - Batch inference for 8 titles, duration: 0:00:11.841098
2025-03-19 15:03:48,432 - Batch inference for 8 titles, duration: 0:00:11.878516
llama3.2-prompt1-1:  33%|███▎      | 2/6 [00:11<00:19,  4.92s/it]2025-03-19 15:03:49,028 - Batch inference for 8 titles, duration: 0:00:12.474347
llama3.2-prompt1-1:  50%|█████     | 3/6 [00:12<00:08,  2.94s/it]llama3.2-prompt1-1: 100%|██████████| 6/6 [00:12<00:00,  2.08s/it]
2025-03-19 15:03:49,030 - ThreadPoolExecutor for llama3.2-prompt1-1, duration: 0:00:12.476666
2025-03-19 15:03:49,040 - Assigned results for llama3.2-prompt1-1, duration: 0:00:00.009683
2025-03-19 15:03:49,042 - Wrote result JSON for llama3.2-prompt1-1, duration: 0:00:00.002385
llama3.2-prompt1-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:03:49,078 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:49,104 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:49,105 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:49,106 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:49,107 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:49,108 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:56,789 - Batch inference for 3 titles, duration: 0:00:07.745486
2025-03-19 15:03:57,053 - Batch inference for 8 titles, duration: 0:00:08.009194
2025-03-19 15:03:57,950 - Batch inference for 8 titles, duration: 0:00:08.906536
2025-03-19 15:03:58,080 - Batch inference for 8 titles, duration: 0:00:09.037112
llama3.2-prompt1-2:  17%|█▋        | 1/6 [00:09<00:45,  9.04s/it]2025-03-19 15:03:59,116 - Batch inference for 8 titles, duration: 0:00:10.072257
2025-03-19 15:03:59,413 - Batch inference for 8 titles, duration: 0:00:10.369756
llama3.2-prompt1-2:  50%|█████     | 3/6 [00:10<00:08,  2.84s/it]llama3.2-prompt1-2: 100%|██████████| 6/6 [00:10<00:00,  1.73s/it]
2025-03-19 15:03:59,414 - ThreadPoolExecutor for llama3.2-prompt1-2, duration: 0:00:10.370934
2025-03-19 15:03:59,423 - Assigned results for llama3.2-prompt1-2, duration: 0:00:00.009494
2025-03-19 15:03:59,425 - Wrote result JSON for llama3.2-prompt1-2, duration: 0:00:00.001849
llama3.2-prompt1-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:03:59,469 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:59,488 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:59,489 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:59,491 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:59,491 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:03:59,492 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:06,912 - Batch inference for 8 titles, duration: 0:00:07.486670
llama3.2-prompt1-3:  17%|█▋        | 1/6 [00:07<00:37,  7.49s/it]2025-03-19 15:04:07,297 - Batch inference for 3 titles, duration: 0:00:07.870946
2025-03-19 15:04:07,904 - Batch inference for 8 titles, duration: 0:00:08.477990
2025-03-19 15:04:08,613 - Batch inference for 8 titles, duration: 0:00:09.186992
llama3.2-prompt1-3:  33%|███▎      | 2/6 [00:09<00:16,  4.08s/it]2025-03-19 15:04:08,963 - Batch inference for 8 titles, duration: 0:00:09.536340
2025-03-19 15:04:09,690 - Batch inference for 8 titles, duration: 0:00:10.263923
llama3.2-prompt1-3:  50%|█████     | 3/6 [00:10<00:08,  2.71s/it]llama3.2-prompt1-3: 100%|██████████| 6/6 [00:10<00:00,  1.71s/it]
2025-03-19 15:04:09,691 - ThreadPoolExecutor for llama3.2-prompt1-3, duration: 0:00:10.265061
2025-03-19 15:04:09,700 - Assigned results for llama3.2-prompt1-3, duration: 0:00:00.009500
2025-03-19 15:04:09,705 - Wrote result JSON for llama3.2-prompt1-3, duration: 0:00:00.004691
llama3.2-prompt1-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:04:09,747 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:09,766 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:09,768 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:09,769 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:09,771 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:09,772 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:17,074 - Batch inference for 8 titles, duration: 0:00:07.368240
2025-03-19 15:04:17,550 - Batch inference for 3 titles, duration: 0:00:07.843450
2025-03-19 15:04:19,594 - Batch inference for 8 titles, duration: 0:00:09.887946
2025-03-19 15:04:19,957 - Batch inference for 8 titles, duration: 0:00:10.251536
2025-03-19 15:04:20,773 - Batch inference for 8 titles, duration: 0:00:11.067202
2025-03-19 15:04:20,806 - Batch inference for 8 titles, duration: 0:00:11.100053
llama3.2-prompt1-4:  17%|█▋        | 1/6 [00:11<00:55, 11.10s/it]llama3.2-prompt1-4: 100%|██████████| 6/6 [00:11<00:00,  1.85s/it]
2025-03-19 15:04:20,806 - ThreadPoolExecutor for llama3.2-prompt1-4, duration: 0:00:11.100906
2025-03-19 15:04:20,816 - Assigned results for llama3.2-prompt1-4, duration: 0:00:00.009478
2025-03-19 15:04:20,818 - Wrote result JSON for llama3.2-prompt1-4, duration: 0:00:00.001992
llama3.2-prompt1-5:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:04:20,855 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:20,874 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:20,875 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:20,876 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:20,906 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:20,906 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:29,121 - Batch inference for 3 titles, duration: 0:00:08.301595
2025-03-19 15:04:29,315 - Batch inference for 8 titles, duration: 0:00:08.496384
2025-03-19 15:04:29,996 - Batch inference for 8 titles, duration: 0:00:09.177547
2025-03-19 15:04:30,945 - Batch inference for 8 titles, duration: 0:00:10.126012
2025-03-19 15:04:31,882 - Batch inference for 8 titles, duration: 0:00:11.062949
2025-03-19 15:04:32,990 - Batch inference for 8 titles, duration: 0:00:12.171907
llama3.2-prompt1-5:  17%|█▋        | 1/6 [00:12<01:00, 12.17s/it]llama3.2-prompt1-5: 100%|██████████| 6/6 [00:12<00:00,  2.03s/it]
2025-03-19 15:04:32,991 - ThreadPoolExecutor for llama3.2-prompt1-5, duration: 0:00:12.172765
2025-03-19 15:04:33,001 - Assigned results for llama3.2-prompt1-5, duration: 0:00:00.009473
2025-03-19 15:04:33,004 - Wrote result JSON for llama3.2-prompt1-5, duration: 0:00:00.002724
llama3.2-prompt1-6:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:04:33,040 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:33,066 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:33,067 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:33,069 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:33,070 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:33,072 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:39,563 - Batch inference for 3 titles, duration: 0:00:06.558528
2025-03-19 15:04:40,407 - Batch inference for 8 titles, duration: 0:00:07.402584
2025-03-19 15:04:40,874 - Batch inference for 8 titles, duration: 0:00:07.869789
llama3.2-prompt1-6:  17%|█▋        | 1/6 [00:07<00:39,  7.87s/it]2025-03-19 15:04:44,802 - Batch inference for 8 titles, duration: 0:00:11.797969
llama3.2-prompt1-6:  33%|███▎      | 2/6 [00:11<00:22,  5.55s/it]2025-03-19 15:04:45,150 - Batch inference for 8 titles, duration: 0:00:12.145930
2025-03-19 15:04:45,338 - Batch inference for 8 titles, duration: 0:00:12.333976
llama3.2-prompt1-6:  50%|█████     | 3/6 [00:12<00:09,  3.26s/it]llama3.2-prompt1-6: 100%|██████████| 6/6 [00:12<00:00,  2.06s/it]
2025-03-19 15:04:45,339 - ThreadPoolExecutor for llama3.2-prompt1-6, duration: 0:00:12.335188
2025-03-19 15:04:45,348 - Assigned results for llama3.2-prompt1-6, duration: 0:00:00.009491
2025-03-19 15:04:45,351 - Wrote result JSON for llama3.2-prompt1-6, duration: 0:00:00.002372
llama3.2-prompt1-7:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:04:45,391 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:45,410 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:45,412 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:45,413 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:45,414 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:45,415 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:51,822 - Batch inference for 3 titles, duration: 0:00:06.469755
2025-03-19 15:04:53,874 - Batch inference for 8 titles, duration: 0:00:08.521585
2025-03-19 15:04:54,443 - Batch inference for 8 titles, duration: 0:00:09.090536
2025-03-19 15:04:55,568 - Batch inference for 8 titles, duration: 0:00:10.215815
2025-03-19 15:04:55,635 - Batch inference for 8 titles, duration: 0:00:10.283531
llama3.2-prompt1-7:  17%|█▋        | 1/6 [00:10<00:51, 10.28s/it]2025-03-19 15:04:55,927 - Batch inference for 8 titles, duration: 0:00:10.575138
llama3.2-prompt1-7:  50%|█████     | 3/6 [00:10<00:08,  2.77s/it]llama3.2-prompt1-7: 100%|██████████| 6/6 [00:10<00:00,  1.76s/it]
2025-03-19 15:04:55,928 - ThreadPoolExecutor for llama3.2-prompt1-7, duration: 0:00:10.576477
2025-03-19 15:04:55,938 - Assigned results for llama3.2-prompt1-7, duration: 0:00:00.009667
2025-03-19 15:04:55,940 - Wrote result JSON for llama3.2-prompt1-7, duration: 0:00:00.001913
llama3.2-prompt1-8:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:04:55,980 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:55,982 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:56,005 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:56,007 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:56,009 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:04:56,010 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:03,694 - Batch inference for 8 titles, duration: 0:00:07.754062
llama3.2-prompt1-8:  17%|█▋        | 1/6 [00:07<00:38,  7.75s/it]2025-03-19 15:05:04,732 - Batch inference for 3 titles, duration: 0:00:08.790595
2025-03-19 15:05:05,844 - Batch inference for 8 titles, duration: 0:00:09.903231
llama3.2-prompt1-8:  33%|███▎      | 2/6 [00:09<00:17,  4.46s/it]2025-03-19 15:05:05,966 - Batch inference for 8 titles, duration: 0:00:10.025627
2025-03-19 15:05:06,950 - Batch inference for 8 titles, duration: 0:00:11.009619
2025-03-19 15:05:07,998 - Batch inference for 8 titles, duration: 0:00:12.057395
llama3.2-prompt1-8:  50%|█████     | 3/6 [00:12<00:10,  3.41s/it]llama3.2-prompt1-8: 100%|██████████| 6/6 [00:12<00:00,  2.01s/it]
2025-03-19 15:05:07,998 - ThreadPoolExecutor for llama3.2-prompt1-8, duration: 0:00:12.058582
2025-03-19 15:05:08,008 - Assigned results for llama3.2-prompt1-8, duration: 0:00:00.009619
2025-03-19 15:05:08,011 - Wrote result JSON for llama3.2-prompt1-8, duration: 0:00:00.002047
llama3.2-prompt1-9:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:05:08,065 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:08,085 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:08,086 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:08,087 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:08,089 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:08,090 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:15,895 - Batch inference for 8 titles, duration: 0:00:07.883755
2025-03-19 15:05:16,060 - Batch inference for 3 titles, duration: 0:00:08.048071
2025-03-19 15:05:16,252 - Batch inference for 8 titles, duration: 0:00:08.240210
2025-03-19 15:05:18,476 - Batch inference for 8 titles, duration: 0:00:10.465160
2025-03-19 15:05:19,079 - Batch inference for 8 titles, duration: 0:00:11.067895
llama3.2-prompt1-9:  17%|█▋        | 1/6 [00:11<00:55, 11.07s/it]2025-03-19 15:05:19,897 - Batch inference for 8 titles, duration: 0:00:11.885688
llama3.2-prompt1-9:  50%|█████     | 3/6 [00:11<00:09,  3.17s/it]llama3.2-prompt1-9: 100%|██████████| 6/6 [00:11<00:00,  1.98s/it]
2025-03-19 15:05:19,898 - ThreadPoolExecutor for llama3.2-prompt1-9, duration: 0:00:11.887100
2025-03-19 15:05:19,907 - Assigned results for llama3.2-prompt1-9, duration: 0:00:00.009438
2025-03-19 15:05:19,910 - Wrote result JSON for llama3.2-prompt1-9, duration: 0:00:00.002153
2025-03-19 15:05:19,910 - Processing model: mistral
2025-03-19 15:05:19,910 - Warming up mistral
time=2025-03-19T15:05:20.210+01:00 level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-c84f8fdc-c01c-c96b-dfb4-dce174caf888 library=cuda total="79.1 GiB" available="73.4 GiB"
time=2025-03-19T15:05:20.211+01:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 gpu=GPU-c84f8fdc-c01c-c96b-dfb4-dce174caf888 parallel=8 available=78859337728 required="7.4 GiB"
time=2025-03-19T15:05:20.487+01:00 level=INFO source=server.go:104 msg="system memory" total="503.5 GiB" free="481.2 GiB" free_swap="14.8 GiB"
time=2025-03-19T15:05:20.487+01:00 level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[73.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="7.4 GiB" memory.required.partial="7.4 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[7.4 GiB]" memory.weights.total="5.7 GiB" memory.weights.repeating="5.6 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.1 GiB"
time=2025-03-19T15:05:20.490+01:00 level=INFO source=server.go:376 msg="starting llama server" cmd="/pfs/data5/software_uc2/bwhpc/common/cs/ollama/0.5.7/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 16384 --batch-size 512 --n-gpu-layers 33 --threads 64 --parallel 8 --port 35563"
time=2025-03-19T15:05:20.490+01:00 level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-03-19T15:05:20.490+01:00 level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
time=2025-03-19T15:05:20.490+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
time=2025-03-19T15:05:20.522+01:00 level=INFO source=runner.go:936 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 PCIe, compute capability 9.0, VMM: yes
time=2025-03-19T15:05:20.564+01:00 level=INFO source=runner.go:937 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=64
time=2025-03-19T15:05:20.564+01:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:35563"
time=2025-03-19T15:05:20.741+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA H100 PCIe) - 75206 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 771
llm_load_vocab: token to piece cache size = 0.1731 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32768
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.25 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = Mistral-7B-Instruct-v0.3
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 781 '<0x0A>'
llm_load_print_meta: EOG token        = 2 '</s>'
llm_load_print_meta: max token length = 48
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =  3850.02 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    72.00 MiB
llama_new_context_with_model: n_seq_max     = 8
llama_new_context_with_model: n_ctx         = 16384
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 4096
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     1.12 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB
llama_new_context_with_model: graph nodes  = 1030
llama_new_context_with_model: graph splits = 2
time=2025-03-19T15:05:21.996+01:00 level=INFO source=server.go:594 msg="llama runner started in 1.51 seconds"
2025-03-19 15:05:22,085 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
mistral-no_prompt-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:05:25,139 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:25,212 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:25,213 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:25,213 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:25,214 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:25,214 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:31,076 - Batch inference for 8 titles, duration: 0:00:05.974896
2025-03-19 15:05:34,038 - Batch inference for 3 titles, duration: 0:00:08.936809
2025-03-19 15:05:34,969 - Batch inference for 8 titles, duration: 0:00:09.867493
2025-03-19 15:05:35,416 - Batch inference for 8 titles, duration: 0:00:10.314404
2025-03-19 15:05:36,535 - Batch inference for 8 titles, duration: 0:00:11.433976
2025-03-19 15:05:38,065 - Batch inference for 8 titles, duration: 0:00:12.964053
mistral-no_prompt-0:  17%|█▋        | 1/6 [00:12<01:04, 12.96s/it]mistral-no_prompt-0: 100%|██████████| 6/6 [00:12<00:00,  2.16s/it]
2025-03-19 15:05:38,066 - ThreadPoolExecutor for mistral-no_prompt-0, duration: 0:00:12.964956
2025-03-19 15:05:38,075 - Assigned results for mistral-no_prompt-0, duration: 0:00:00.009499
2025-03-19 15:05:38,078 - Wrote result JSON for mistral-no_prompt-0, duration: 0:00:00.002340
mistral-no_prompt-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:05:38,098 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:38,115 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:38,116 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:38,116 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:38,117 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:38,117 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:47,415 - Batch inference for 3 titles, duration: 0:00:09.335812
2025-03-19 15:05:48,036 - Batch inference for 8 titles, duration: 0:00:09.957487
2025-03-19 15:05:49,128 - Batch inference for 8 titles, duration: 0:00:11.049050
2025-03-19 15:05:49,636 - Batch inference for 8 titles, duration: 0:00:11.557109
2025-03-19 15:05:52,259 - Batch inference for 8 titles, duration: 0:00:14.180416
2025-03-19 15:05:54,063 - Batch inference for 8 titles, duration: 0:00:15.984799
mistral-no_prompt-1:  17%|█▋        | 1/6 [00:15<01:19, 15.98s/it]mistral-no_prompt-1: 100%|██████████| 6/6 [00:15<00:00,  2.66s/it]
2025-03-19 15:05:54,064 - ThreadPoolExecutor for mistral-no_prompt-1, duration: 0:00:15.985566
2025-03-19 15:05:54,073 - Assigned results for mistral-no_prompt-1, duration: 0:00:00.009681
2025-03-19 15:05:54,077 - Wrote result JSON for mistral-no_prompt-1, duration: 0:00:00.003166
mistral-no_prompt-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:05:54,101 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:54,101 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:54,121 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:54,122 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:54,122 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:05:54,123 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:02,151 - Batch inference for 8 titles, duration: 0:00:08.073501
2025-03-19 15:06:02,472 - Batch inference for 3 titles, duration: 0:00:08.394103
2025-03-19 15:06:04,155 - Batch inference for 8 titles, duration: 0:00:10.077552
2025-03-19 15:06:04,515 - Batch inference for 8 titles, duration: 0:00:10.437331
2025-03-19 15:06:05,011 - Batch inference for 8 titles, duration: 0:00:10.934176
mistral-no_prompt-2:  17%|█▋        | 1/6 [00:10<00:54, 10.93s/it]2025-03-19 15:06:06,383 - Batch inference for 8 titles, duration: 0:00:12.305054
mistral-no_prompt-2:  83%|████████▎ | 5/6 [00:12<00:01,  1.92s/it]mistral-no_prompt-2: 100%|██████████| 6/6 [00:12<00:00,  2.05s/it]
2025-03-19 15:06:06,383 - ThreadPoolExecutor for mistral-no_prompt-2, duration: 0:00:12.306582
2025-03-19 15:06:06,393 - Assigned results for mistral-no_prompt-2, duration: 0:00:00.009534
2025-03-19 15:06:06,395 - Wrote result JSON for mistral-no_prompt-2, duration: 0:00:00.001664
mistral-no_prompt-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:06:06,417 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:06,434 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:06,435 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:06,435 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:06,435 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:06,436 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:12,745 - Batch inference for 3 titles, duration: 0:00:06.349082
2025-03-19 15:06:16,843 - Batch inference for 8 titles, duration: 0:00:10.447391
2025-03-19 15:06:16,876 - Batch inference for 8 titles, duration: 0:00:10.479919
2025-03-19 15:06:18,556 - Batch inference for 8 titles, duration: 0:00:12.160526
2025-03-19 15:06:18,655 - Batch inference for 8 titles, duration: 0:00:12.259503
2025-03-19 15:06:19,428 - Batch inference for 8 titles, duration: 0:00:13.032232
mistral-no_prompt-3:  17%|█▋        | 1/6 [00:13<01:05, 13.03s/it]mistral-no_prompt-3: 100%|██████████| 6/6 [00:13<00:00,  2.17s/it]
2025-03-19 15:06:19,428 - ThreadPoolExecutor for mistral-no_prompt-3, duration: 0:00:13.033125
2025-03-19 15:06:19,438 - Assigned results for mistral-no_prompt-3, duration: 0:00:00.009535
2025-03-19 15:06:19,440 - Wrote result JSON for mistral-no_prompt-3, duration: 0:00:00.001269
mistral-no_prompt-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:06:19,464 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:19,464 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:19,485 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:19,485 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:19,485 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:19,486 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:25,892 - Batch inference for 8 titles, duration: 0:00:06.451910
mistral-no_prompt-4:  17%|█▋        | 1/6 [00:06<00:32,  6.45s/it]2025-03-19 15:06:26,982 - Batch inference for 3 titles, duration: 0:00:07.541014
2025-03-19 15:06:27,529 - Batch inference for 8 titles, duration: 0:00:08.088610
2025-03-19 15:06:28,420 - Batch inference for 8 titles, duration: 0:00:08.980002
2025-03-19 15:06:30,372 - Batch inference for 8 titles, duration: 0:00:10.932084
2025-03-19 15:06:30,669 - Batch inference for 8 titles, duration: 0:00:11.228749
mistral-no_prompt-4:  33%|███▎      | 2/6 [00:11<00:21,  5.47s/it]mistral-no_prompt-4: 100%|██████████| 6/6 [00:11<00:00,  1.87s/it]
2025-03-19 15:06:30,670 - ThreadPoolExecutor for mistral-no_prompt-4, duration: 0:00:11.229815
2025-03-19 15:06:30,679 - Assigned results for mistral-no_prompt-4, duration: 0:00:00.009497
2025-03-19 15:06:30,681 - Wrote result JSON for mistral-no_prompt-4, duration: 0:00:00.001721
mistral-no_prompt-5:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:06:30,706 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:30,723 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:30,723 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:30,724 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:30,725 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:30,725 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:40,421 - Batch inference for 8 titles, duration: 0:00:09.739163
2025-03-19 15:06:41,320 - Batch inference for 8 titles, duration: 0:00:10.638126
2025-03-19 15:06:41,659 - Batch inference for 8 titles, duration: 0:00:10.977212
2025-03-19 15:06:41,789 - Batch inference for 8 titles, duration: 0:00:11.107628
2025-03-19 15:06:42,087 - Batch inference for 3 titles, duration: 0:00:11.404830
2025-03-19 15:06:43,208 - Batch inference for 8 titles, duration: 0:00:12.526567
mistral-no_prompt-5:  17%|█▋        | 1/6 [00:12<01:02, 12.53s/it]mistral-no_prompt-5: 100%|██████████| 6/6 [00:12<00:00,  2.09s/it]
2025-03-19 15:06:43,209 - ThreadPoolExecutor for mistral-no_prompt-5, duration: 0:00:12.527330
2025-03-19 15:06:43,218 - Assigned results for mistral-no_prompt-5, duration: 0:00:00.009492
2025-03-19 15:06:43,220 - Wrote result JSON for mistral-no_prompt-5, duration: 0:00:00.001842
mistral-no_prompt-6:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:06:43,242 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:43,259 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:43,260 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:43,260 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:43,261 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:43,261 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:51,776 - Batch inference for 8 titles, duration: 0:00:08.555054
2025-03-19 15:06:53,798 - Batch inference for 3 titles, duration: 0:00:10.576664
2025-03-19 15:06:55,384 - Batch inference for 8 titles, duration: 0:00:12.163130
2025-03-19 15:06:55,394 - Batch inference for 8 titles, duration: 0:00:12.172974
2025-03-19 15:06:55,769 - Batch inference for 8 titles, duration: 0:00:12.547946
mistral-no_prompt-6:  17%|█▋        | 1/6 [00:12<01:02, 12.55s/it]2025-03-19 15:06:55,965 - Batch inference for 8 titles, duration: 0:00:12.743750
mistral-no_prompt-6:  50%|█████     | 3/6 [00:12<00:09,  3.33s/it]mistral-no_prompt-6: 100%|██████████| 6/6 [00:12<00:00,  2.12s/it]
2025-03-19 15:06:55,965 - ThreadPoolExecutor for mistral-no_prompt-6, duration: 0:00:12.744962
2025-03-19 15:06:55,975 - Assigned results for mistral-no_prompt-6, duration: 0:00:00.009620
2025-03-19 15:06:55,977 - Wrote result JSON for mistral-no_prompt-6, duration: 0:00:00.001691
mistral-no_prompt-7:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:06:55,999 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:56,016 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:56,017 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:56,017 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:56,018 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:06:56,018 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:03,770 - Batch inference for 3 titles, duration: 0:00:07.791241
2025-03-19 15:07:05,367 - Batch inference for 8 titles, duration: 0:00:09.388339
2025-03-19 15:07:06,416 - Batch inference for 8 titles, duration: 0:00:10.438142
mistral-no_prompt-7:  17%|█▋        | 1/6 [00:10<00:52, 10.44s/it]2025-03-19 15:07:06,850 - Batch inference for 8 titles, duration: 0:00:10.871901
2025-03-19 15:07:06,948 - Batch inference for 8 titles, duration: 0:00:10.970085
2025-03-19 15:07:08,226 - Batch inference for 8 titles, duration: 0:00:12.248678
mistral-no_prompt-7:  33%|███▎      | 2/6 [00:12<00:21,  5.36s/it]mistral-no_prompt-7: 100%|██████████| 6/6 [00:12<00:00,  2.04s/it]
2025-03-19 15:07:08,227 - ThreadPoolExecutor for mistral-no_prompt-7, duration: 0:00:12.249736
2025-03-19 15:07:08,237 - Assigned results for mistral-no_prompt-7, duration: 0:00:00.009477
2025-03-19 15:07:08,238 - Wrote result JSON for mistral-no_prompt-7, duration: 0:00:00.001514
mistral-no_prompt-8:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:07:08,261 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:08,278 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:08,278 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:08,279 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:08,279 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:08,279 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:14,452 - Batch inference for 8 titles, duration: 0:00:06.213728
mistral-no_prompt-8:  17%|█▋        | 1/6 [00:06<00:31,  6.21s/it]2025-03-19 15:07:17,135 - Batch inference for 3 titles, duration: 0:00:08.895341
2025-03-19 15:07:17,488 - Batch inference for 8 titles, duration: 0:00:09.248589
2025-03-19 15:07:18,206 - Batch inference for 8 titles, duration: 0:00:09.966609
2025-03-19 15:07:19,623 - Batch inference for 8 titles, duration: 0:00:11.383300
2025-03-19 15:07:20,549 - Batch inference for 8 titles, duration: 0:00:12.310344
mistral-no_prompt-8:  33%|███▎      | 2/6 [00:12<00:24,  6.14s/it]mistral-no_prompt-8: 100%|██████████| 6/6 [00:12<00:00,  2.05s/it]
2025-03-19 15:07:20,550 - ThreadPoolExecutor for mistral-no_prompt-8, duration: 0:00:12.311376
2025-03-19 15:07:20,560 - Assigned results for mistral-no_prompt-8, duration: 0:00:00.009475
2025-03-19 15:07:20,561 - Wrote result JSON for mistral-no_prompt-8, duration: 0:00:00.001522
mistral-no_prompt-9:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:07:20,582 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:20,599 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:20,599 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:20,600 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:20,600 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:20,601 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:29,261 - Batch inference for 3 titles, duration: 0:00:08.698346
2025-03-19 15:07:30,129 - Batch inference for 8 titles, duration: 0:00:09.567187
2025-03-19 15:07:32,409 - Batch inference for 8 titles, duration: 0:00:11.847050
2025-03-19 15:07:32,560 - Batch inference for 8 titles, duration: 0:00:11.997739
2025-03-19 15:07:33,299 - Batch inference for 8 titles, duration: 0:00:12.737498
2025-03-19 15:07:35,089 - Batch inference for 8 titles, duration: 0:00:14.527349
mistral-no_prompt-9:  17%|█▋        | 1/6 [00:14<01:12, 14.53s/it]mistral-no_prompt-9: 100%|██████████| 6/6 [00:14<00:00,  2.42s/it]
2025-03-19 15:07:35,090 - ThreadPoolExecutor for mistral-no_prompt-9, duration: 0:00:14.528143
2025-03-19 15:07:35,099 - Assigned results for mistral-no_prompt-9, duration: 0:00:00.009501
2025-03-19 15:07:35,101 - Wrote result JSON for mistral-no_prompt-9, duration: 0:00:00.001573
2025-03-19 15:07:35,101 - Wrote prompt prompt1, duration: 0:00:00.000197
mistral-prompt1-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:07:35,515 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:35,516 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:35,516 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:35,829 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:35,861 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:35,861 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:40,477 - Batch inference for 3 titles, duration: 0:00:05.374148
2025-03-19 15:07:45,242 - Batch inference for 8 titles, duration: 0:00:10.139772
2025-03-19 15:07:45,299 - Batch inference for 8 titles, duration: 0:00:10.197164
2025-03-19 15:07:45,987 - Batch inference for 8 titles, duration: 0:00:10.885055
2025-03-19 15:07:47,103 - Batch inference for 8 titles, duration: 0:00:12.000956
mistral-prompt1-0:  17%|█▋        | 1/6 [00:12<01:00, 12.00s/it]2025-03-19 15:07:47,267 - Batch inference for 8 titles, duration: 0:00:12.164451
mistral-prompt1-0:  83%|████████▎ | 5/6 [00:12<00:01,  1.82s/it]mistral-prompt1-0: 100%|██████████| 6/6 [00:12<00:00,  2.03s/it]
2025-03-19 15:07:47,267 - ThreadPoolExecutor for mistral-prompt1-0, duration: 0:00:12.165820
2025-03-19 15:07:47,277 - Assigned results for mistral-prompt1-0, duration: 0:00:00.009428
2025-03-19 15:07:47,279 - Wrote result JSON for mistral-prompt1-0, duration: 0:00:00.001737
mistral-prompt1-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:07:47,307 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:47,308 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:47,330 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:47,331 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:47,331 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:47,331 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:51,697 - Batch inference for 3 titles, duration: 0:00:04.416976
2025-03-19 15:07:53,085 - Batch inference for 8 titles, duration: 0:00:05.805524
mistral-prompt1-1:  17%|█▋        | 1/6 [00:05<00:29,  5.80s/it]2025-03-19 15:07:56,343 - Batch inference for 8 titles, duration: 0:00:09.063970
mistral-prompt1-1:  33%|███▎      | 2/6 [00:09<00:17,  4.31s/it]2025-03-19 15:07:56,502 - Batch inference for 8 titles, duration: 0:00:09.221817
2025-03-19 15:07:57,444 - Batch inference for 8 titles, duration: 0:00:10.163938
2025-03-19 15:07:57,524 - Batch inference for 8 titles, duration: 0:00:10.243948
mistral-prompt1-1:  50%|█████     | 3/6 [00:10<00:08,  2.88s/it]mistral-prompt1-1: 100%|██████████| 6/6 [00:10<00:00,  1.71s/it]
2025-03-19 15:07:57,524 - ThreadPoolExecutor for mistral-prompt1-1, duration: 0:00:10.245211
2025-03-19 15:07:57,534 - Assigned results for mistral-prompt1-1, duration: 0:00:00.009457
2025-03-19 15:07:57,536 - Wrote result JSON for mistral-prompt1-1, duration: 0:00:00.001451
mistral-prompt1-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:07:57,561 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:57,562 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:57,584 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:57,584 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:57,585 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:07:57,586 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:02,086 - Batch inference for 3 titles, duration: 0:00:04.549117
2025-03-19 15:08:02,377 - Batch inference for 8 titles, duration: 0:00:04.840843
2025-03-19 15:08:03,935 - Batch inference for 8 titles, duration: 0:00:06.399530
mistral-prompt1-2:  17%|█▋        | 1/6 [00:06<00:31,  6.40s/it]2025-03-19 15:08:06,262 - Batch inference for 8 titles, duration: 0:00:08.725152
2025-03-19 15:08:07,128 - Batch inference for 8 titles, duration: 0:00:09.592190
mistral-prompt1-2:  33%|███▎      | 2/6 [00:09<00:18,  4.51s/it]2025-03-19 15:08:08,201 - Batch inference for 8 titles, duration: 0:00:10.664477
mistral-prompt1-2:  50%|█████     | 3/6 [00:10<00:08,  2.94s/it]mistral-prompt1-2: 100%|██████████| 6/6 [00:10<00:00,  1.78s/it]
2025-03-19 15:08:08,201 - ThreadPoolExecutor for mistral-prompt1-2, duration: 0:00:10.665621
2025-03-19 15:08:08,211 - Assigned results for mistral-prompt1-2, duration: 0:00:00.009486
2025-03-19 15:08:08,213 - Wrote result JSON for mistral-prompt1-2, duration: 0:00:00.001910
mistral-prompt1-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:08:08,236 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:08,257 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:08,258 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:08,258 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:08,258 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:08,259 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:12,218 - Batch inference for 3 titles, duration: 0:00:04.003327
2025-03-19 15:08:13,703 - Batch inference for 8 titles, duration: 0:00:05.489694
2025-03-19 15:08:14,262 - Batch inference for 8 titles, duration: 0:00:06.047861
2025-03-19 15:08:16,944 - Batch inference for 8 titles, duration: 0:00:08.729690
2025-03-19 15:08:17,837 - Batch inference for 8 titles, duration: 0:00:09.623625
2025-03-19 15:08:18,162 - Batch inference for 8 titles, duration: 0:00:09.948326
mistral-prompt1-3:  17%|█▋        | 1/6 [00:09<00:49,  9.95s/it]mistral-prompt1-3: 100%|██████████| 6/6 [00:09<00:00,  1.66s/it]
2025-03-19 15:08:18,162 - ThreadPoolExecutor for mistral-prompt1-3, duration: 0:00:09.949084
2025-03-19 15:08:18,172 - Assigned results for mistral-prompt1-3, duration: 0:00:00.009514
2025-03-19 15:08:18,174 - Wrote result JSON for mistral-prompt1-3, duration: 0:00:00.001658
mistral-prompt1-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:08:18,198 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:18,218 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:18,219 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:18,219 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:18,220 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:18,220 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:23,350 - Batch inference for 3 titles, duration: 0:00:05.174391
2025-03-19 15:08:23,984 - Batch inference for 8 titles, duration: 0:00:05.809350
2025-03-19 15:08:25,720 - Batch inference for 8 titles, duration: 0:00:07.545063
2025-03-19 15:08:28,571 - Batch inference for 8 titles, duration: 0:00:10.396206
2025-03-19 15:08:29,110 - Batch inference for 8 titles, duration: 0:00:10.935208
2025-03-19 15:08:30,034 - Batch inference for 8 titles, duration: 0:00:11.859884
mistral-prompt1-4:  17%|█▋        | 1/6 [00:11<00:59, 11.86s/it]mistral-prompt1-4: 100%|██████████| 6/6 [00:11<00:00,  1.98s/it]
2025-03-19 15:08:30,035 - ThreadPoolExecutor for mistral-prompt1-4, duration: 0:00:11.860656
2025-03-19 15:08:30,045 - Assigned results for mistral-prompt1-4, duration: 0:00:00.009664
2025-03-19 15:08:30,046 - Wrote result JSON for mistral-prompt1-4, duration: 0:00:00.001696
mistral-prompt1-5:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:08:30,070 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:30,090 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:30,091 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:30,091 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:30,092 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:30,092 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:35,001 - Batch inference for 3 titles, duration: 0:00:04.953420
2025-03-19 15:08:35,841 - Batch inference for 8 titles, duration: 0:00:05.793884
mistral-prompt1-5:  17%|█▋        | 1/6 [00:05<00:28,  5.79s/it]2025-03-19 15:08:35,899 - Batch inference for 8 titles, duration: 0:00:05.851752
2025-03-19 15:08:39,577 - Batch inference for 8 titles, duration: 0:00:09.529999
2025-03-19 15:08:40,484 - Batch inference for 8 titles, duration: 0:00:10.436564
2025-03-19 15:08:42,021 - Batch inference for 8 titles, duration: 0:00:11.974335
mistral-prompt1-5:  33%|███▎      | 2/6 [00:11<00:24,  6.02s/it]mistral-prompt1-5: 100%|██████████| 6/6 [00:11<00:00,  2.00s/it]
2025-03-19 15:08:42,022 - ThreadPoolExecutor for mistral-prompt1-5, duration: 0:00:11.975472
2025-03-19 15:08:42,035 - Assigned results for mistral-prompt1-5, duration: 0:00:00.012777
2025-03-19 15:08:42,037 - Wrote result JSON for mistral-prompt1-5, duration: 0:00:00.001923
mistral-prompt1-6:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:08:42,065 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:42,065 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:42,089 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:42,089 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:42,089 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:42,090 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:46,594 - Batch inference for 3 titles, duration: 0:00:04.554662
2025-03-19 15:08:47,909 - Batch inference for 8 titles, duration: 0:00:05.871255
mistral-prompt1-6:  17%|█▋        | 1/6 [00:05<00:29,  5.87s/it]2025-03-19 15:08:50,576 - Batch inference for 8 titles, duration: 0:00:08.537240
2025-03-19 15:08:51,792 - Batch inference for 8 titles, duration: 0:00:09.753345
2025-03-19 15:08:52,070 - Batch inference for 8 titles, duration: 0:00:10.032074
mistral-prompt1-6:  33%|███▎      | 2/6 [00:10<00:19,  4.86s/it]2025-03-19 15:08:52,378 - Batch inference for 8 titles, duration: 0:00:10.339664
mistral-prompt1-6:  67%|██████▋   | 4/6 [00:10<00:03,  1.91s/it]mistral-prompt1-6: 100%|██████████| 6/6 [00:10<00:00,  1.72s/it]
2025-03-19 15:08:52,379 - ThreadPoolExecutor for mistral-prompt1-6, duration: 0:00:10.341474
2025-03-19 15:08:52,389 - Assigned results for mistral-prompt1-6, duration: 0:00:00.009521
2025-03-19 15:08:52,391 - Wrote result JSON for mistral-prompt1-6, duration: 0:00:00.001831
mistral-prompt1-7:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:08:52,417 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:52,437 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:52,438 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:52,438 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:52,439 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:52,439 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:08:57,284 - Batch inference for 8 titles, duration: 0:00:04.892889
2025-03-19 15:08:57,784 - Batch inference for 8 titles, duration: 0:00:05.392405
mistral-prompt1-7:  17%|█▋        | 1/6 [00:05<00:26,  5.39s/it]2025-03-19 15:08:58,504 - Batch inference for 3 titles, duration: 0:00:06.112047
2025-03-19 15:09:00,109 - Batch inference for 8 titles, duration: 0:00:07.717245
2025-03-19 15:09:01,393 - Batch inference for 8 titles, duration: 0:00:09.001419
mistral-prompt1-7:  50%|█████     | 3/6 [00:09<00:08,  2.73s/it]2025-03-19 15:09:02,016 - Batch inference for 8 titles, duration: 0:00:09.624101
mistral-prompt1-7:  83%|████████▎ | 5/6 [00:09<00:01,  1.49s/it]mistral-prompt1-7: 100%|██████████| 6/6 [00:09<00:00,  1.60s/it]
2025-03-19 15:09:02,016 - ThreadPoolExecutor for mistral-prompt1-7, duration: 0:00:09.625521
2025-03-19 15:09:02,026 - Assigned results for mistral-prompt1-7, duration: 0:00:00.009452
2025-03-19 15:09:02,028 - Wrote result JSON for mistral-prompt1-7, duration: 0:00:00.001874
mistral-prompt1-8:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:09:02,052 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:02,072 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:02,073 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:02,073 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:02,074 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:02,074 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:06,838 - Batch inference for 8 titles, duration: 0:00:04.809082
mistral-prompt1-8:  17%|█▋        | 1/6 [00:04<00:24,  4.81s/it]2025-03-19 15:09:07,496 - Batch inference for 3 titles, duration: 0:00:05.466256
2025-03-19 15:09:11,338 - Batch inference for 8 titles, duration: 0:00:09.308739
2025-03-19 15:09:11,982 - Batch inference for 8 titles, duration: 0:00:09.952918
2025-03-19 15:09:12,433 - Batch inference for 8 titles, duration: 0:00:10.404337
2025-03-19 15:09:13,037 - Batch inference for 8 titles, duration: 0:00:11.008584
mistral-prompt1-8:  33%|███▎      | 2/6 [00:11<00:22,  5.63s/it]mistral-prompt1-8: 100%|██████████| 6/6 [00:11<00:00,  1.83s/it]
2025-03-19 15:09:13,038 - ThreadPoolExecutor for mistral-prompt1-8, duration: 0:00:11.009679
2025-03-19 15:09:13,048 - Assigned results for mistral-prompt1-8, duration: 0:00:00.009448
2025-03-19 15:09:13,049 - Wrote result JSON for mistral-prompt1-8, duration: 0:00:00.001344
mistral-prompt1-9:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:09:13,072 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:13,093 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:13,093 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:13,094 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:13,094 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:13,095 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:17,630 - Batch inference for 3 titles, duration: 0:00:04.579717
2025-03-19 15:09:18,821 - Batch inference for 8 titles, duration: 0:00:05.771786
mistral-prompt1-9:  17%|█▋        | 1/6 [00:05<00:28,  5.77s/it]2025-03-19 15:09:19,855 - Batch inference for 8 titles, duration: 0:00:06.804688
2025-03-19 15:09:21,822 - Batch inference for 8 titles, duration: 0:00:08.771697
2025-03-19 15:09:22,380 - Batch inference for 8 titles, duration: 0:00:09.329877
2025-03-19 15:09:24,353 - Batch inference for 8 titles, duration: 0:00:11.302824
mistral-prompt1-9:  33%|███▎      | 2/6 [00:11<00:22,  5.63s/it]mistral-prompt1-9: 100%|██████████| 6/6 [00:11<00:00,  1.88s/it]
2025-03-19 15:09:24,353 - ThreadPoolExecutor for mistral-prompt1-9, duration: 0:00:11.303866
2025-03-19 15:09:24,363 - Assigned results for mistral-prompt1-9, duration: 0:00:00.009547
2025-03-19 15:09:24,365 - Wrote result JSON for mistral-prompt1-9, duration: 0:00:00.001637
2025-03-19 15:09:24,365 - Processing model: deepseek-r1
2025-03-19 15:09:24,365 - Warming up deepseek-r1
time=2025-03-19T15:09:24.858+01:00 level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-c84f8fdc-c01c-c96b-dfb4-dce174caf888 library=cuda total="79.1 GiB" available="65.8 GiB"
time=2025-03-19T15:09:24.858+01:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 gpu=GPU-c84f8fdc-c01c-c96b-dfb4-dce174caf888 parallel=8 available=70632669184 required="6.5 GiB"
time=2025-03-19T15:09:25.127+01:00 level=INFO source=server.go:104 msg="system memory" total="503.5 GiB" free="480.8 GiB" free_swap="14.8 GiB"
time=2025-03-19T15:09:25.127+01:00 level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[65.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="942.0 MiB" memory.graph.partial="1.1 GiB"
time=2025-03-19T15:09:25.129+01:00 level=INFO source=server.go:376 msg="starting llama server" cmd="/pfs/data5/software_uc2/bwhpc/common/cs/ollama/0.5.7/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 --ctx-size 16384 --batch-size 512 --n-gpu-layers 29 --threads 64 --parallel 8 --port 40413"
time=2025-03-19T15:09:25.130+01:00 level=INFO source=sched.go:449 msg="loaded runners" count=3
time=2025-03-19T15:09:25.130+01:00 level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
time=2025-03-19T15:09:25.130+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
time=2025-03-19T15:09:25.162+01:00 level=INFO source=runner.go:936 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 PCIe, compute capability 9.0, VMM: yes
time=2025-03-19T15:09:25.210+01:00 level=INFO source=runner.go:937 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=64
time=2025-03-19T15:09:25.211+01:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:40413"
time=2025-03-19T15:09:25.381+01:00 level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA H100 PCIe) - 67360 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/ma/ma_ma/ma_ssiu/.ollama/models/blobs/sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 7B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 3584
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 28
llm_load_print_meta: n_head_kv        = 4
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 7
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 18944
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 7.62 B
llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 7B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 28 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 29/29 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =  4168.09 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
llama_new_context_with_model: n_seq_max     = 8
llama_new_context_with_model: n_ctx         = 16384
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 4096
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   896.00 MiB
llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     4.75 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   956.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    39.01 MiB
llama_new_context_with_model: graph nodes  = 986
llama_new_context_with_model: graph splits = 2
time=2025-03-19T15:09:32.152+01:00 level=INFO source=server.go:594 msg="llama runner started in 7.02 seconds"
2025-03-19 15:09:32,242 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
deepseek-r1-no_prompt-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:09:32,764 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:32,833 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:32,835 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:32,837 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:32,838 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:32,840 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:09:54,701 - Batch inference for 8 titles, duration: 0:00:21.995629
deepseek-r1-no_prompt-0:  17%|█▋        | 1/6 [00:21<01:49, 21.99s/it]2025-03-19 15:09:59,874 - Batch inference for 8 titles, duration: 0:00:27.167309
2025-03-19 15:10:01,734 - Batch inference for 8 titles, duration: 0:00:29.027512
2025-03-19 15:10:02,887 - Batch inference for 3 titles, duration: 0:00:30.180608
2025-03-19 15:10:16,806 - Batch inference for 8 titles, duration: 0:00:44.100408
deepseek-r1-no_prompt-0:  33%|███▎      | 2/6 [00:44<01:28, 22.06s/it]2025-03-19 15:10:17,985 - Batch inference for 8 titles, duration: 0:00:45.278551
deepseek-r1-no_prompt-0:  67%|██████▋   | 4/6 [00:45<00:17,  8.60s/it]deepseek-r1-no_prompt-0: 100%|██████████| 6/6 [00:45<00:00,  7.55s/it]
2025-03-19 15:10:17,985 - ThreadPoolExecutor for deepseek-r1-no_prompt-0, duration: 0:00:45.279958
2025-03-19 15:10:17,995 - Assigned results for deepseek-r1-no_prompt-0, duration: 0:00:00.009596
2025-03-19 15:10:17,997 - Wrote result JSON for deepseek-r1-no_prompt-0, duration: 0:00:00.001791
deepseek-r1-no_prompt-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:10:18,062 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:18,148 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:18,150 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:18,152 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:18,154 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:18,155 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:41,939 - Batch inference for 3 titles, duration: 0:00:23.940515
2025-03-19 15:10:44,092 - Batch inference for 8 titles, duration: 0:00:26.093783
2025-03-19 15:10:46,985 - Batch inference for 8 titles, duration: 0:00:28.986250
2025-03-19 15:10:49,341 - Batch inference for 8 titles, duration: 0:00:31.343115
2025-03-19 15:10:51,864 - Batch inference for 8 titles, duration: 0:00:33.866417
2025-03-19 15:10:51,894 - Batch inference for 8 titles, duration: 0:00:33.896392
deepseek-r1-no_prompt-1:  17%|█▋        | 1/6 [00:33<02:49, 33.90s/it]deepseek-r1-no_prompt-1: 100%|██████████| 6/6 [00:33<00:00,  5.65s/it]
2025-03-19 15:10:51,895 - ThreadPoolExecutor for deepseek-r1-no_prompt-1, duration: 0:00:33.897180
2025-03-19 15:10:51,904 - Assigned results for deepseek-r1-no_prompt-1, duration: 0:00:00.009514
2025-03-19 15:10:51,906 - Wrote result JSON for deepseek-r1-no_prompt-1, duration: 0:00:00.001679
deepseek-r1-no_prompt-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:10:51,946 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:51,974 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:51,975 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:51,977 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:51,979 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:10:51,981 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:09,501 - Batch inference for 8 titles, duration: 0:00:17.593823
2025-03-19 15:11:11,272 - Batch inference for 3 titles, duration: 0:00:19.365010
2025-03-19 15:11:12,184 - Batch inference for 8 titles, duration: 0:00:20.276784
2025-03-19 15:11:15,574 - Batch inference for 8 titles, duration: 0:00:23.667533
deepseek-r1-no_prompt-2:  17%|█▋        | 1/6 [00:23<01:58, 23.67s/it]2025-03-19 15:11:15,613 - Batch inference for 8 titles, duration: 0:00:23.705914
2025-03-19 15:11:17,114 - Batch inference for 8 titles, duration: 0:00:25.206848
deepseek-r1-no_prompt-2:  50%|█████     | 3/6 [00:25<00:20,  6.71s/it]deepseek-r1-no_prompt-2: 100%|██████████| 6/6 [00:25<00:00,  4.20s/it]
2025-03-19 15:11:17,114 - ThreadPoolExecutor for deepseek-r1-no_prompt-2, duration: 0:00:25.208033
2025-03-19 15:11:17,124 - Assigned results for deepseek-r1-no_prompt-2, duration: 0:00:00.009683
2025-03-19 15:11:17,126 - Wrote result JSON for deepseek-r1-no_prompt-2, duration: 0:00:00.001712
deepseek-r1-no_prompt-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:11:17,165 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:17,193 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:17,195 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:17,196 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:17,198 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:17,200 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:38,119 - Batch inference for 8 titles, duration: 0:00:20.992411
2025-03-19 15:11:38,559 - Batch inference for 8 titles, duration: 0:00:21.431988
2025-03-19 15:11:39,515 - Batch inference for 3 titles, duration: 0:00:22.387748
2025-03-19 15:11:41,925 - Batch inference for 8 titles, duration: 0:00:24.797716
2025-03-19 15:11:44,348 - Batch inference for 8 titles, duration: 0:00:27.221127
2025-03-19 15:11:56,837 - Batch inference for 8 titles, duration: 0:00:39.710433
deepseek-r1-no_prompt-3:  17%|█▋        | 1/6 [00:39<03:18, 39.71s/it]deepseek-r1-no_prompt-3: 100%|██████████| 6/6 [00:39<00:00,  6.62s/it]
2025-03-19 15:11:56,838 - ThreadPoolExecutor for deepseek-r1-no_prompt-3, duration: 0:00:39.711233
2025-03-19 15:11:56,847 - Assigned results for deepseek-r1-no_prompt-3, duration: 0:00:00.009413
2025-03-19 15:11:56,849 - Wrote result JSON for deepseek-r1-no_prompt-3, duration: 0:00:00.001671
deepseek-r1-no_prompt-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:11:56,914 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:56,916 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:57,002 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:57,004 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:57,005 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:11:57,007 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:12:16,288 - Batch inference for 3 titles, duration: 0:00:19.437385
2025-03-19 15:12:25,114 - Batch inference for 8 titles, duration: 0:00:28.263739
2025-03-19 15:12:25,637 - Batch inference for 8 titles, duration: 0:00:28.787654
2025-03-19 15:12:26,744 - Batch inference for 8 titles, duration: 0:00:29.894373
deepseek-r1-no_prompt-4:  17%|█▋        | 1/6 [00:29<02:29, 29.89s/it]2025-03-19 15:12:27,857 - Batch inference for 8 titles, duration: 0:00:31.006863
deepseek-r1-no_prompt-4:  50%|█████     | 3/6 [00:31<00:24,  8.16s/it]2025-03-19 15:12:33,500 - Batch inference for 8 titles, duration: 0:00:36.650472
deepseek-r1-no_prompt-4:  83%|████████▎ | 5/6 [00:36<00:05,  5.42s/it]deepseek-r1-no_prompt-4: 100%|██████████| 6/6 [00:36<00:00,  6.11s/it]
2025-03-19 15:12:33,502 - ThreadPoolExecutor for deepseek-r1-no_prompt-4, duration: 0:00:36.652950
2025-03-19 15:12:33,512 - Assigned results for deepseek-r1-no_prompt-4, duration: 0:00:00.009644
2025-03-19 15:12:33,514 - Wrote result JSON for deepseek-r1-no_prompt-4, duration: 0:00:00.001720
deepseek-r1-no_prompt-5:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:12:33,553 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:12:33,638 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:12:33,640 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:12:33,641 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:12:33,643 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:12:33,645 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:12:56,613 - Batch inference for 8 titles, duration: 0:00:23.098900
2025-03-19 15:12:58,431 - Batch inference for 8 titles, duration: 0:00:24.916416
2025-03-19 15:12:58,872 - Batch inference for 8 titles, duration: 0:00:25.358103
deepseek-r1-no_prompt-5:  17%|█▋        | 1/6 [00:25<02:06, 25.36s/it]2025-03-19 15:12:59,339 - Batch inference for 3 titles, duration: 0:00:25.824047
2025-03-19 15:13:02,806 - Batch inference for 8 titles, duration: 0:00:29.291016
2025-03-19 15:13:06,771 - Batch inference for 8 titles, duration: 0:00:33.256150
deepseek-r1-no_prompt-5:  50%|█████     | 3/6 [00:33<00:28,  9.50s/it]deepseek-r1-no_prompt-5: 100%|██████████| 6/6 [00:33<00:00,  5.54s/it]
2025-03-19 15:13:06,771 - ThreadPoolExecutor for deepseek-r1-no_prompt-5, duration: 0:00:33.257324
2025-03-19 15:13:06,781 - Assigned results for deepseek-r1-no_prompt-5, duration: 0:00:00.009439
2025-03-19 15:13:06,783 - Wrote result JSON for deepseek-r1-no_prompt-5, duration: 0:00:00.001590
deepseek-r1-no_prompt-6:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:13:06,822 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:06,850 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:06,852 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:06,853 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:06,855 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:06,856 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:30,626 - Batch inference for 8 titles, duration: 0:00:23.842713
2025-03-19 15:13:31,457 - Batch inference for 3 titles, duration: 0:00:24.672750
2025-03-19 15:13:34,506 - Batch inference for 8 titles, duration: 0:00:27.722889
deepseek-r1-no_prompt-6:  17%|█▋        | 1/6 [00:27<02:18, 27.72s/it]2025-03-19 15:13:35,177 - Batch inference for 8 titles, duration: 0:00:28.393905
2025-03-19 15:13:36,501 - Batch inference for 8 titles, duration: 0:00:29.717657
2025-03-19 15:13:40,300 - Batch inference for 8 titles, duration: 0:00:33.517156
deepseek-r1-no_prompt-6:  33%|███▎      | 2/6 [00:33<00:59, 14.82s/it]deepseek-r1-no_prompt-6: 100%|██████████| 6/6 [00:33<00:00,  5.59s/it]
2025-03-19 15:13:40,301 - ThreadPoolExecutor for deepseek-r1-no_prompt-6, duration: 0:00:33.518165
2025-03-19 15:13:40,311 - Assigned results for deepseek-r1-no_prompt-6, duration: 0:00:00.009482
2025-03-19 15:13:40,313 - Wrote result JSON for deepseek-r1-no_prompt-6, duration: 0:00:00.002387
deepseek-r1-no_prompt-7:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:13:40,356 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:40,385 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:40,386 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:40,387 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:40,389 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:13:40,391 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:04,401 - Batch inference for 8 titles, duration: 0:00:24.086902
2025-03-19 15:14:05,195 - Batch inference for 8 titles, duration: 0:00:24.880343
2025-03-19 15:14:08,368 - Batch inference for 8 titles, duration: 0:00:28.053889
2025-03-19 15:14:12,290 - Batch inference for 3 titles, duration: 0:00:31.975806
2025-03-19 15:14:12,472 - Batch inference for 8 titles, duration: 0:00:32.158082
deepseek-r1-no_prompt-7:  17%|█▋        | 1/6 [00:32<02:40, 32.16s/it]2025-03-19 15:14:13,969 - Batch inference for 8 titles, duration: 0:00:33.654782
deepseek-r1-no_prompt-7:  33%|███▎      | 2/6 [00:33<00:56, 14.12s/it]deepseek-r1-no_prompt-7: 100%|██████████| 6/6 [00:33<00:00,  5.61s/it]
2025-03-19 15:14:13,969 - ThreadPoolExecutor for deepseek-r1-no_prompt-7, duration: 0:00:33.655795
2025-03-19 15:14:13,979 - Assigned results for deepseek-r1-no_prompt-7, duration: 0:00:00.009707
2025-03-19 15:14:13,982 - Wrote result JSON for deepseek-r1-no_prompt-7, duration: 0:00:00.002422
deepseek-r1-no_prompt-8:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:14:14,019 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:14,047 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:14,049 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:14,051 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:14,052 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:14,054 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:34,964 - Batch inference for 8 titles, duration: 0:00:20.981436
2025-03-19 15:14:36,324 - Batch inference for 3 titles, duration: 0:00:22.341045
2025-03-19 15:14:37,969 - Batch inference for 8 titles, duration: 0:00:23.986764
2025-03-19 15:14:39,145 - Batch inference for 8 titles, duration: 0:00:25.162784
2025-03-19 15:14:46,738 - Batch inference for 8 titles, duration: 0:00:32.755601
2025-03-19 15:14:48,120 - Batch inference for 8 titles, duration: 0:00:34.138201
deepseek-r1-no_prompt-8:  17%|█▋        | 1/6 [00:34<02:50, 34.14s/it]deepseek-r1-no_prompt-8: 100%|██████████| 6/6 [00:34<00:00,  5.69s/it]
2025-03-19 15:14:48,121 - ThreadPoolExecutor for deepseek-r1-no_prompt-8, duration: 0:00:34.139107
2025-03-19 15:14:48,131 - Assigned results for deepseek-r1-no_prompt-8, duration: 0:00:00.009417
2025-03-19 15:14:48,133 - Wrote result JSON for deepseek-r1-no_prompt-8, duration: 0:00:00.001822
deepseek-r1-no_prompt-9:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:14:48,174 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:48,202 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:48,204 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:48,205 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:48,207 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:14:48,208 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:06,318 - Batch inference for 3 titles, duration: 0:00:18.184263
2025-03-19 15:15:14,064 - Batch inference for 8 titles, duration: 0:00:25.930483
deepseek-r1-no_prompt-9:  17%|█▋        | 1/6 [00:25<02:09, 25.93s/it]2025-03-19 15:15:14,571 - Batch inference for 8 titles, duration: 0:00:26.437157
2025-03-19 15:15:15,612 - Batch inference for 8 titles, duration: 0:00:27.478611
2025-03-19 15:15:19,371 - Batch inference for 8 titles, duration: 0:00:31.237235
2025-03-19 15:15:20,171 - Batch inference for 8 titles, duration: 0:00:32.037624
deepseek-r1-no_prompt-9:  33%|███▎      | 2/6 [00:32<00:57, 14.27s/it]deepseek-r1-no_prompt-9: 100%|██████████| 6/6 [00:32<00:00,  5.34s/it]
2025-03-19 15:15:20,172 - ThreadPoolExecutor for deepseek-r1-no_prompt-9, duration: 0:00:32.038655
2025-03-19 15:15:20,181 - Assigned results for deepseek-r1-no_prompt-9, duration: 0:00:00.009435
2025-03-19 15:15:20,184 - Wrote result JSON for deepseek-r1-no_prompt-9, duration: 0:00:00.002361
2025-03-19 15:15:20,184 - Wrote prompt prompt1, duration: 0:00:00.000207
deepseek-r1-prompt1-0:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:15:20,365 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:20,884 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:20,885 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:20,887 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:20,889 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:20,890 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:32,451 - Batch inference for 8 titles, duration: 0:00:12.266502
deepseek-r1-prompt1-0:  17%|█▋        | 1/6 [00:12<01:01, 12.27s/it]2025-03-19 15:15:33,233 - Batch inference for 3 titles, duration: 0:00:13.047667
2025-03-19 15:15:36,257 - Batch inference for 8 titles, duration: 0:00:16.072455
2025-03-19 15:15:36,636 - Batch inference for 8 titles, duration: 0:00:16.450651
2025-03-19 15:15:38,367 - Batch inference for 8 titles, duration: 0:00:18.182897
deepseek-r1-prompt1-0:  33%|███▎      | 2/6 [00:18<00:34,  8.53s/it]2025-03-19 15:15:40,075 - Batch inference for 8 titles, duration: 0:00:19.889923
deepseek-r1-prompt1-0:  50%|█████     | 3/6 [00:19<00:16,  5.42s/it]deepseek-r1-prompt1-0: 100%|██████████| 6/6 [00:19<00:00,  3.32s/it]
2025-03-19 15:15:40,076 - ThreadPoolExecutor for deepseek-r1-prompt1-0, duration: 0:00:19.892079
2025-03-19 15:15:40,086 - Assigned results for deepseek-r1-prompt1-0, duration: 0:00:00.009638
2025-03-19 15:15:40,088 - Wrote result JSON for deepseek-r1-prompt1-0, duration: 0:00:00.001631
deepseek-r1-prompt1-1:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:15:40,139 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:40,166 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:40,167 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:40,169 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:40,170 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:40,172 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:50,603 - Batch inference for 8 titles, duration: 0:00:10.514290
2025-03-19 15:15:53,799 - Batch inference for 8 titles, duration: 0:00:13.709820
2025-03-19 15:15:54,901 - Batch inference for 8 titles, duration: 0:00:14.812405
2025-03-19 15:15:56,781 - Batch inference for 8 titles, duration: 0:00:16.692588
2025-03-19 15:15:57,459 - Batch inference for 8 titles, duration: 0:00:17.370865
deepseek-r1-prompt1-1:  17%|█▋        | 1/6 [00:17<01:26, 17.37s/it]2025-03-19 15:15:59,313 - Batch inference for 3 titles, duration: 0:00:19.224098
deepseek-r1-prompt1-1: 100%|██████████| 6/6 [00:19<00:00,  2.46s/it]deepseek-r1-prompt1-1: 100%|██████████| 6/6 [00:19<00:00,  3.20s/it]
2025-03-19 15:15:59,314 - ThreadPoolExecutor for deepseek-r1-prompt1-1, duration: 0:00:19.225708
2025-03-19 15:15:59,323 - Assigned results for deepseek-r1-prompt1-1, duration: 0:00:00.009517
2025-03-19 15:15:59,326 - Wrote result JSON for deepseek-r1-prompt1-1, duration: 0:00:00.002113
deepseek-r1-prompt1-2:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:15:59,374 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:59,401 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:59,402 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:59,403 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:59,404 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:15:59,405 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:10,161 - Batch inference for 8 titles, duration: 0:00:10.834869
deepseek-r1-prompt1-2:  17%|█▋        | 1/6 [00:10<00:54, 10.83s/it]2025-03-19 15:16:11,687 - Batch inference for 8 titles, duration: 0:00:12.359993
2025-03-19 15:16:14,725 - Batch inference for 3 titles, duration: 0:00:15.398235
2025-03-19 15:16:16,186 - Batch inference for 8 titles, duration: 0:00:16.859439
2025-03-19 15:16:18,372 - Batch inference for 8 titles, duration: 0:00:19.045561
deepseek-r1-prompt1-2:  33%|███▎      | 2/6 [00:19<00:37,  9.29s/it]2025-03-19 15:16:18,467 - Batch inference for 8 titles, duration: 0:00:19.139997
deepseek-r1-prompt1-2: 100%|██████████| 6/6 [00:19<00:00,  3.19s/it]
2025-03-19 15:16:18,467 - ThreadPoolExecutor for deepseek-r1-prompt1-2, duration: 0:00:19.141490
2025-03-19 15:16:18,477 - Assigned results for deepseek-r1-prompt1-2, duration: 0:00:00.009377
2025-03-19 15:16:18,479 - Wrote result JSON for deepseek-r1-prompt1-2, duration: 0:00:00.001565
deepseek-r1-prompt1-3:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:16:18,520 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:18,548 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:18,549 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:18,551 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:18,552 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:18,554 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:28,189 - Batch inference for 8 titles, duration: 0:00:09.709399
2025-03-19 15:16:30,531 - Batch inference for 8 titles, duration: 0:00:12.051127
2025-03-19 15:16:31,296 - Batch inference for 3 titles, duration: 0:00:12.816416
2025-03-19 15:16:32,150 - Batch inference for 8 titles, duration: 0:00:13.670794
deepseek-r1-prompt1-3:  17%|█▋        | 1/6 [00:13<01:08, 13.67s/it]2025-03-19 15:16:32,603 - Batch inference for 8 titles, duration: 0:00:14.123992
2025-03-19 15:16:34,690 - Batch inference for 8 titles, duration: 0:00:16.210492
deepseek-r1-prompt1-3:  33%|███▎      | 2/6 [00:16<00:28,  7.12s/it]deepseek-r1-prompt1-3: 100%|██████████| 6/6 [00:16<00:00,  2.70s/it]
2025-03-19 15:16:34,690 - ThreadPoolExecutor for deepseek-r1-prompt1-3, duration: 0:00:16.211483
2025-03-19 15:16:34,700 - Assigned results for deepseek-r1-prompt1-3, duration: 0:00:00.009475
2025-03-19 15:16:34,702 - Wrote result JSON for deepseek-r1-prompt1-3, duration: 0:00:00.001742
deepseek-r1-prompt1-4:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:16:34,746 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:34,773 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:34,774 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:34,776 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:34,777 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:34,779 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:42,843 - Batch inference for 8 titles, duration: 0:00:08.140000
2025-03-19 15:16:44,447 - Batch inference for 8 titles, duration: 0:00:09.744690
2025-03-19 15:16:46,337 - Batch inference for 8 titles, duration: 0:00:11.635005
deepseek-r1-prompt1-4:  17%|█▋        | 1/6 [00:11<00:58, 11.63s/it]2025-03-19 15:16:47,724 - Batch inference for 8 titles, duration: 0:00:13.020778
2025-03-19 15:16:47,944 - Batch inference for 8 titles, duration: 0:00:13.241394
deepseek-r1-prompt1-4:  50%|█████     | 3/6 [00:13<00:10,  3.61s/it]2025-03-19 15:16:53,946 - Batch inference for 3 titles, duration: 0:00:19.242273
deepseek-r1-prompt1-4: 100%|██████████| 6/6 [00:19<00:00,  2.62s/it]deepseek-r1-prompt1-4: 100%|██████████| 6/6 [00:19<00:00,  3.21s/it]
2025-03-19 15:16:53,946 - ThreadPoolExecutor for deepseek-r1-prompt1-4, duration: 0:00:19.243982
2025-03-19 15:16:53,956 - Assigned results for deepseek-r1-prompt1-4, duration: 0:00:00.009508
2025-03-19 15:16:53,957 - Wrote result JSON for deepseek-r1-prompt1-4, duration: 0:00:00.001369
deepseek-r1-prompt1-5:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:16:53,997 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:54,024 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:54,026 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:54,028 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:54,030 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:16:54,032 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:05,687 - Batch inference for 8 titles, duration: 0:00:11.728921
2025-03-19 15:17:08,543 - Batch inference for 8 titles, duration: 0:00:14.584581
2025-03-19 15:17:09,525 - Batch inference for 8 titles, duration: 0:00:15.566505
2025-03-19 15:17:13,111 - Batch inference for 3 titles, duration: 0:00:19.152064
2025-03-19 15:17:14,278 - Batch inference for 8 titles, duration: 0:00:20.319636
2025-03-19 15:17:17,918 - Batch inference for 8 titles, duration: 0:00:23.960422
deepseek-r1-prompt1-5:  17%|█▋        | 1/6 [00:23<01:59, 23.96s/it]deepseek-r1-prompt1-5: 100%|██████████| 6/6 [00:23<00:00,  3.99s/it]
2025-03-19 15:17:17,919 - ThreadPoolExecutor for deepseek-r1-prompt1-5, duration: 0:00:23.961283
2025-03-19 15:17:17,928 - Assigned results for deepseek-r1-prompt1-5, duration: 0:00:00.009517
2025-03-19 15:17:17,930 - Wrote result JSON for deepseek-r1-prompt1-5, duration: 0:00:00.001479
deepseek-r1-prompt1-6:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:17:17,972 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:17,999 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:18,000 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:18,001 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:18,002 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:18,004 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:24,976 - Batch inference for 8 titles, duration: 0:00:07.045282
2025-03-19 15:17:27,979 - Batch inference for 8 titles, duration: 0:00:10.048398
deepseek-r1-prompt1-6:  17%|█▋        | 1/6 [00:10<00:50, 10.05s/it]2025-03-19 15:17:29,843 - Batch inference for 8 titles, duration: 0:00:11.912058
2025-03-19 15:17:31,355 - Batch inference for 8 titles, duration: 0:00:13.424367
2025-03-19 15:17:31,664 - Batch inference for 3 titles, duration: 0:00:13.732788
2025-03-19 15:17:33,554 - Batch inference for 8 titles, duration: 0:00:15.622905
deepseek-r1-prompt1-6:  33%|███▎      | 2/6 [00:15<00:29,  7.42s/it]deepseek-r1-prompt1-6: 100%|██████████| 6/6 [00:15<00:00,  2.60s/it]
2025-03-19 15:17:33,556 - ThreadPoolExecutor for deepseek-r1-prompt1-6, duration: 0:00:15.625785
2025-03-19 15:17:33,566 - Assigned results for deepseek-r1-prompt1-6, duration: 0:00:00.009635
2025-03-19 15:17:33,568 - Wrote result JSON for deepseek-r1-prompt1-6, duration: 0:00:00.001739
deepseek-r1-prompt1-7:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:17:33,613 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:33,639 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:33,641 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:33,643 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:33,644 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:33,646 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:44,277 - Batch inference for 8 titles, duration: 0:00:10.707631
2025-03-19 15:17:46,164 - Batch inference for 8 titles, duration: 0:00:12.595403
2025-03-19 15:17:46,540 - Batch inference for 8 titles, duration: 0:00:12.971946
deepseek-r1-prompt1-7:  17%|█▋        | 1/6 [00:12<01:04, 12.97s/it]2025-03-19 15:17:50,766 - Batch inference for 8 titles, duration: 0:00:17.197769
deepseek-r1-prompt1-7:  33%|███▎      | 2/6 [00:17<00:31,  7.83s/it]2025-03-19 15:17:50,828 - Batch inference for 3 titles, duration: 0:00:17.258747
2025-03-19 15:17:51,946 - Batch inference for 8 titles, duration: 0:00:18.376845
deepseek-r1-prompt1-7:  67%|██████▋   | 4/6 [00:18<00:06,  3.29s/it]deepseek-r1-prompt1-7: 100%|██████████| 6/6 [00:18<00:00,  3.06s/it]
2025-03-19 15:17:51,946 - ThreadPoolExecutor for deepseek-r1-prompt1-7, duration: 0:00:18.378290
2025-03-19 15:17:51,956 - Assigned results for deepseek-r1-prompt1-7, duration: 0:00:00.009397
2025-03-19 15:17:51,958 - Wrote result JSON for deepseek-r1-prompt1-7, duration: 0:00:00.001711
deepseek-r1-prompt1-8:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:17:52,009 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:52,036 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:52,038 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:52,039 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:52,041 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:17:52,042 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:18:04,933 - Batch inference for 8 titles, duration: 0:00:12.974247
2025-03-19 15:18:07,308 - Batch inference for 8 titles, duration: 0:00:15.349313
2025-03-19 15:18:07,879 - Batch inference for 8 titles, duration: 0:00:15.920835
2025-03-19 15:18:09,525 - Batch inference for 8 titles, duration: 0:00:17.565980
2025-03-19 15:18:09,953 - Batch inference for 3 titles, duration: 0:00:17.993939
2025-03-19 15:18:12,144 - Batch inference for 8 titles, duration: 0:00:20.185462
deepseek-r1-prompt1-8:  17%|█▋        | 1/6 [00:20<01:40, 20.18s/it]deepseek-r1-prompt1-8: 100%|██████████| 6/6 [00:20<00:00,  3.36s/it]
2025-03-19 15:18:12,144 - ThreadPoolExecutor for deepseek-r1-prompt1-8, duration: 0:00:20.186276
2025-03-19 15:18:12,154 - Assigned results for deepseek-r1-prompt1-8, duration: 0:00:00.009467
2025-03-19 15:18:12,156 - Wrote result JSON for deepseek-r1-prompt1-8, duration: 0:00:00.001459
deepseek-r1-prompt1-9:   0%|          | 0/6 [00:00<?, ?it/s]2025-03-19 15:18:12,203 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:18:12,205 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:18:12,237 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:18:12,239 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:18:12,241 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:18:12,242 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-03-19 15:18:24,747 - Batch inference for 3 titles, duration: 0:00:12.590613
2025-03-19 15:18:25,824 - Batch inference for 8 titles, duration: 0:00:13.667576
2025-03-19 15:18:26,093 - Batch inference for 8 titles, duration: 0:00:13.936928
2025-03-19 15:18:28,772 - Batch inference for 8 titles, duration: 0:00:16.615837
2025-03-19 15:18:31,998 - Batch inference for 8 titles, duration: 0:00:19.841486
2025-03-19 15:18:33,335 - Batch inference for 8 titles, duration: 0:00:21.178861
deepseek-r1-prompt1-9:  17%|█▋        | 1/6 [00:21<01:45, 21.18s/it]deepseek-r1-prompt1-9: 100%|██████████| 6/6 [00:21<00:00,  3.53s/it]
2025-03-19 15:18:33,335 - ThreadPoolExecutor for deepseek-r1-prompt1-9, duration: 0:00:21.179733
2025-03-19 15:18:33,345 - Assigned results for deepseek-r1-prompt1-9, duration: 0:00:00.009438
2025-03-19 15:18:33,347 - Wrote result JSON for deepseek-r1-prompt1-9, duration: 0:00:00.001595
2025-03-19 15:18:33,347 - Script completed
