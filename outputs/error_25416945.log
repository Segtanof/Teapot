/pfs/work7/workspace/scratch/ma_ssiu-myspace/teapot/1_optimized.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  sampled_occupation = occupations.groupby("ind").apply(lambda x: x.sample(frac=0.05, random_state=1)).reset_index(drop=True)
2025-03-25 15:15:41,737 - INFO - Script started
2025-03-25 15:15:41,737 - INFO - Processing model: llama3.2:3b
2025-03-25 15:15:41,755 - DEBUG - connect_tcp.started host='127.0.0.1' port=12379 local_address=None timeout=None socket_options=None
2025-03-25 15:15:41,755 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x145e3be0f7d0>
2025-03-25 15:15:41,755 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-25 15:15:41,755 - DEBUG - send_request_headers.complete
2025-03-25 15:15:41,755 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-25 15:15:41,756 - DEBUG - send_request_body.complete
2025-03-25 15:15:41,756 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-25 15:15:41,759 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 25 Mar 2025 14:15:41 GMT'), (b'Content-Length', b'65')])
2025-03-25 15:15:41,760 - INFO - HTTP Request: POST http://127.0.0.1:12379/api/chat "HTTP/1.1 404 Not Found"
2025-03-25 15:15:41,760 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-25 15:15:41,760 - DEBUG - receive_response_body.complete
2025-03-25 15:15:41,760 - DEBUG - response_closed.started
2025-03-25 15:15:41,760 - DEBUG - response_closed.complete
Traceback (most recent call last):
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/teapot/1_optimized.py", line 197, in <module>
    main()
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/teapot/1_optimized.py", line 158, in main
    warmup_model.invoke("Warm-up prompt")
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 307, in invoke
    self.generate_prompt(
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 843, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 683, in generate
    self._generate_with_cache(
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 908, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_ollama/chat_models.py", line 701, in _generate
    final_chunk = self._chat_stream_with_aggregation(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_ollama/chat_models.py", line 602, in _chat_stream_with_aggregation
    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/langchain_ollama/chat_models.py", line 589, in _create_chat_stream
    yield from self._client.chat(**chat_params)
  File "/pfs/work7/workspace/scratch/ma_ssiu-myspace/.conda/envs/test/lib/python3.11/site-packages/ollama/_client.py", line 168, in inner
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: model "llama3.2:3b" not found, try pulling it first (status code: 404)
